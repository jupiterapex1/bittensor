{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before running this notebook\n",
    "\n",
    "Make sure that your Jupyter Notebook or Lab is running in the Bittensor environment. You can do so by first activating your environment `source ~/.bittensor/env/bin/activate` and then adding your notebook to the kernel `python -m ipykernel install --user --name=myenv`. You can then run your notebook and select the `bittensor` kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bittensor\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from synapses.gpt2 import GPT2LMSynapse\n",
    "from miners.TEXT.gpt2_genesis.gpt2_genesis import Miner\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "from bittensor.utils.model_utils import ModelToolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a Bittensor-trained GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load up the configs\n",
    "config = bittensor.neuron.Neuron.default_config()\n",
    "config.neuron.multiprocessing = False\n",
    "config.neuron.debug = True\n",
    "config.receptor.do_backoff = False\n",
    "config.subtensor.network = 'kusanagi'\n",
    "\n",
    "config.synapse = GPT2LMSynapse.default_config().synapse\n",
    "config.synapse.n_layer = 48 #1\n",
    "config.synapse.n_inner = 2048\n",
    "config.synapse.n_head = 16\n",
    "\n",
    "config.router = GPT2LMSynapse.default_config().router\n",
    "config.miner = Miner.default_config().miner\n",
    "config.synapse.device = torch.device('cpu')\n",
    "\n",
    "config.miner.trial_uid = 'gpugpt03'\n",
    "\n",
    "# Set up tensorboard fullpath\n",
    "full_path = '{}/{}/{}'.format(config.miner.root_dir, config.miner.name, config.miner.trial_uid)\n",
    "config.miner.full_path = os.path.expanduser(full_path)\n",
    "\n",
    "#print ( bittensor.config.Config.toString(config) )\n",
    "\n",
    "# Load the model from the saved location under miners/gpt2-genesis/...\n",
    "model = GPT2LMSynapse( config )\n",
    "checkpoint = torch.load(\"{}/model.torch\".format(config.miner.full_path), map_location=config.synapse.device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      " doctor house. do.. routing parts,.lat. the cou and of man other is all.\"? was tables. Lexingtonkay his. Revelation than. too the, was not Shards... CONC. Fred... did. supervised? gather. True? and the loveddon., who on.. package. more. developed theAr intermediate.. they which the... John. gave? Lethal do. the Sasha excluded.. herself hat of mc he he"
     ]
    }
   ],
   "source": [
    "string1 = \"I enjoy walking with my cute dog \"\n",
    " # We use the tokenizer to turn the prompt into tokens. Then it gets formatted correctly.\n",
    "tokenizer = bittensor.__tokenizer__()\n",
    "input_ids = tokenizer.encode(string1, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "length = 0\n",
    "text=\"\"\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "with torch.no_grad():\n",
    "    while length < 100:\n",
    "        print(text, end=\"\", flush=True)\n",
    "        logits = model.local_forward(input_ids, training=False)   \n",
    "        logits = model.target_layer(logits.local_hidden)\n",
    "        logits = logits[..., -1, :]\n",
    "        values, indices = torch.topk(logits, bittensor.__vocab_size__)\n",
    "\n",
    "        # All this is to randomly select from the top 40000 choices with the correct probabilites. \n",
    "        min_values = values[:, -1]\n",
    "        logits = torch.where(logits < min_values, torch.ones_like(logits, dtype=logits.dtype) * -1e10, logits)\n",
    "        log_probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(log_probs, num_samples=1)\n",
    "\n",
    "        #Then we decode the chosen token into a string.\n",
    "        text = tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        length +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.938279914855957"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 108, but ``max_length`` is set to 16.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-5ee03a432350>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.bittensor/env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3127\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3128\u001b[0m             \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3129\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3130\u001b[0m         )\n\u001b[1;32m   3131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.bittensor/env/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0mspaces_between_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     ) -> str:\n\u001b[0;32m--> 708\u001b[0;31m         \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0;31m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.bittensor/env/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mconvert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mskip_special_tokens\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "out = model.transformer.generate(input_ids, max_length=16)\n",
    "print(tokenizer.decode(out, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bittensor",
   "language": "python",
   "name": "bittensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
