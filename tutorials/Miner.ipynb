{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bittensor\n",
    "import os\n",
    "import torch\n",
    "import torch.multiprocessing as mp \n",
    "import time\n",
    "from loguru import logger\n",
    "from termcolor import colored\n",
    "import nest_asyncio \n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set logging levels.\n",
    "bittensor.BITTENSOR_STDOUT_LOGGING_LEVEL = 'SUCCESS' # To screen\n",
    "bittensor.BITTENSOR_FILE_LOGGING_LEVEL = 'TRACE' # To ~/.bittensor/logs.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Init Bittensor** <a class=\"anchor\" id=\"Init-Bittensor\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Wallet** <a class=\"anchor\" id=\"Wallet\"></a>\n",
    "**-- Holds your cryptographic keys**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-09 07:44:42.431 | SUCCESS  | bittensor.wallet:_load_coldkeypub:242 - Loaded coldkey.pub: 0xf8b3da5792a2cc7e05857ae4703aee67133944701ecfa806deefc6516a883560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mLoaded coldkey.pub: 0xf8b3da5792a2cc7e05857ae4703aee67133944701ecfa806deefc6516a883560\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-09 07:44:42.434 | SUCCESS  | bittensor.wallet:_load_hotkey:276 - Loaded hotkey: 0xbc8ca2afe5a3bca80254bc51ab9fd9f88a5891bd3ecb8e10f917170b0043ff69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mLoaded hotkey: 0xbc8ca2afe5a3bca80254bc51ab9fd9f88a5891bd3ecb8e10f917170b0043ff69\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# WALLET: Holds keys to run your miner and control funds.\n",
    "\n",
    "# *****\n",
    "# IMPORTANT: Store the mnemonic for **both** your hot and coldkey \n",
    "# you will need these to recover your keys if you forget your password or lose access to this machine.\n",
    "# ******\n",
    "\n",
    "# Fill in below to name your wallet and keys.\n",
    "YOUR_WALLET_NAME = 'colab'\n",
    "YOUR_HOTKEY_NAME = 'colab_hot'\n",
    "\n",
    "# Fill in below if your need to regenerate your keys.\n",
    "use_mnemonic = False # Set to true for key regeneration.\n",
    "coldkey_mnemonic = \"<to be filled>\".split(' ')\n",
    "hotkey_mnemonic = \"<to be filled>\".split(' ')\n",
    "\n",
    "# Create the wallet object.\n",
    "wallet = bittensor.Wallet(\n",
    "    path = \"~/.bittensor/wallets/\",\n",
    "    name = YOUR_WALLET_NAME,\n",
    "    hotkey = YOUR_HOTKEY_NAME\n",
    ")\n",
    "\n",
    "# Optionally regens/creates your wallet keys.\n",
    "if not wallet.has_coldkeypub:\n",
    "    if use_mnemonic:\n",
    "        wallet.regenerate_coldkey(mnemonic = coldkey_mnemonic, use_password = True)\n",
    "    else:\n",
    "        wallet.create_new_coldkey(n_words = 12, use_password = True )\n",
    "if not wallet.has_hotkey:\n",
    "    if use_mnemonic:\n",
    "        wallet.regenerate_hotkey(mnemonic = hotkey_mnemonic)\n",
    "    else:\n",
    "        wallet.create_new_hotkey(n_words = 12)\n",
    "\n",
    "# Assert before continuing\n",
    "assert wallet.has_hotkey\n",
    "assert wallet.has_coldkeypub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Subtensor**\n",
    "**-- Maintains your blockchain connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "subtensor:\n",
      "  chain_endpoint: null\n",
      "  network: kusanagi\n",
      "wallet:\n",
      "  hotkey: default\n",
      "  name: default\n",
      "  path: ~/.bittensor/wallets/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-09 07:44:42.752 | DEBUG    | bittensor.substrate:onConnecting:283 - Connecting to websocket server {\"peer\": \"tcp4:157.230.11.36:9944\", \"is_secure\": false, \"secure_channel_id\": {}}\n",
      "2021-04-09 07:44:43.161 | DEBUG    | bittensor.substrate:onConnect:295 - Connected. {\"peer\": \"tcp4:157.230.11.36:9944\", \"headers\": {\"connection\": \"Upgrade\", \"sec-websocket-accept\": \"HrZadSQWm5npqqdXtMUEmvcwSPE=\", \"upgrade\": \"websocket\"}, \"version\": 18, \"protocol\": null, \"extensions\": []}\n",
      "2021-04-09 07:44:43.163 | DEBUG    | bittensor.substrate:onOpen:301 - Connection open to websocket established\n",
      "2021-04-09 07:44:43.168 | SUCCESS  | bittensor.subtensor:async_connect:264 - Successfully connected to kusanagi endpoint: 157.230.11.36:9944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSuccessfully connected to kusanagi endpoint: 157.230.11.36:9944\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'subtensor' in locals():\n",
    "    del subtensor\n",
    "subtensor = bittensor.Subtensor(\n",
    "    subtensor_network = 'kusanagi'\n",
    ")\n",
    "print (bittensor.Config.toString(subtensor.config))\n",
    "subtensor.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Metagraph** <a class=\"anchor\" id=\"Metagraph\"></a> \n",
    "**-- Caches chain state as torch objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'metagraph' in locals():\n",
    "    del metagraph\n",
    "metagraph = bittensor.Metagraph(\n",
    "    subtensor = subtensor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bittensor.__logger__.remove()\n",
    "bittensor.__user_logger__.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metagraph.sync() # Pull updates from the chain (Note: is expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chain is at block: 1160072\n",
      "\n",
      "There are 502 subscribed miner neurons \n",
      "\n",
      "These are their uids: \n",
      "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
      "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
      "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
      "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502])\n",
      "\n",
      "150 have set weights in the last 1000 blocks\n",
      "\n",
      "τ365263.6875 is staked\n"
     ]
    }
   ],
   "source": [
    "chain_block = metagraph.block()\n",
    "uids_on_chain = metagraph.uids()\n",
    "n_neurons = torch.max( metagraph.uids() )\n",
    "n_online = torch.numel(torch.where( metagraph.block() - metagraph.lastemit() < 1000 )[0])\n",
    "print ('The chain is at block: {}\\n'.format(metagraph.block()))\n",
    "print ('There are {} subscribed miner neurons \\n'.format(n_neurons))\n",
    "print ('These are their uids: \\n{}\\n'.format(metagraph.uids()))\n",
    "print ('{} have set weights in the last 1000 blocks\\n'.format(n_online))\n",
    "print ('\\u03C4{} is staked'.format(torch.sum(metagraph.S())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dendrite**\n",
    "**-- Makes queries to other peers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dendrite:\n",
      "  multiprocess: false\n",
      "receptor:\n",
      "  backward_timeout: 50\n",
      "  do_backoff: false\n",
      "  forward_timeout: 50\n",
      "  max_backoff: 100\n",
      "  pass_gradients: true\n",
      "wallet:\n",
      "  hotkey: default\n",
      "  name: default\n",
      "  path: ~/.bittensor/wallets/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if 'dendrite' in locals():\n",
    "    del dendrite\n",
    "dendrite = bittensor.Dendrite(\n",
    "    wallet = wallet,\n",
    "    dendrite_multiprocess = False,\n",
    "    receptor_do_backoff = False,\n",
    "    receptor_forward_timeout = 50,\n",
    "    receptor_backward_timeout = 50,\n",
    ")\n",
    "print (bittensor.Config.toString(dendrite.config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Nucleus**  <a class=\"anchor\" id=\"Nucleus\"></a>\n",
    "**-- Your unique machine learning model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GPT2-Nucleus** <a class=\"anchor\" id=\"GPT2-Nucleus\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from types import SimpleNamespace\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "class GPT2Nucleus(torch.nn.Module):\n",
    "    \"\"\" A simple-as-it-gets bittensor nucleus using a GPT2 kernel\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        huggingface_config = GPT2Config( vocab_size=bittensor.__vocab_size__, n_embd=bittensor.__network_dim__, n_layer=2, n_head=1, n_inner=8 )\n",
    "        self.transformer = GPT2Model(huggingface_config)\n",
    "        self.hidden_layer = nn.Linear( bittensor.__network_dim__, bittensor.__network_dim__ )\n",
    "        self.target_layer = nn.Linear( bittensor.__network_dim__, bittensor.__vocab_size__, bias=False )\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # The scores you learn for other neurons in the network.\n",
    "        self.row_weights = torch.ones([1], requires_grad=True)\n",
    "        \n",
    "        # Bittensor components that need to be set before you call remote_forward.\n",
    "        self.metagraph = None\n",
    "        self.dendrite = None\n",
    "    \n",
    "    \n",
    "    def local_forward(self, inputs: torch.LongTensor) -> SimpleNamespace:\n",
    "        \"\"\" Applies a forward pass to the model **without** queries to the network.\n",
    "        \"\"\"\n",
    "        # To be filled.\n",
    "        output = SimpleNamespace()\n",
    "\n",
    "        # Apply our GPT transformer model.\n",
    "        # local_context.shape = [ batch_size, sequence_len, network_dim ]\n",
    "        output.local_context = self.transformer(input_ids=inputs, return_dict=True).last_hidden_state\n",
    "\n",
    "        # Apply our dense layer and project it onto our hidden layer.\n",
    "        # local_hidden.shape = [ batch_size, sequence_len, network_dim ]\n",
    "        output.local_hidden = self.hidden_layer( output.local_context )\n",
    "\n",
    "        # Project to our target dimension.\n",
    "        # local_targets.shape = [ batch_size, sequence_len, vocab_size ]\n",
    "        output.local_targets = self.target_layer( output.local_hidden )\n",
    "\n",
    "        # Compute LM-loss \n",
    "        shift_targets = output.local_targets[..., :-1, :].contiguous()\n",
    "        shift_inputs = inputs[..., 1:].contiguous()\n",
    "        output.local_loss = self.loss_fct(shift_targets.view(-1, shift_targets.size(-1)), shift_inputs.view(-1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def remote_forward( self, inputs: torch.LongTensor, n_to_query:int = 10) -> SimpleNamespace:\n",
    "        \"\"\" Applies a forward pass to the model **with** queries to the network.\n",
    "        \"\"\"\n",
    "        # Sanity checks.\n",
    "        assert self.metagraph != None, 'you must assign model.metagraph, before you run a remote call.'\n",
    "        assert self.dendrite != None, 'you must assign model.dendrite, before you run a remote call.'\n",
    "        \n",
    "        # Run the local part of the model.\n",
    "        output = self.local_forward( inputs )\n",
    "\n",
    "        # Make queries to the network.\n",
    "        output = self.filter_and_make_queries( output, inputs, n_to_query )\n",
    "        \n",
    "        # Compute the distillation loss between the local and remote context (produced by the network query.)\n",
    "        output.distillation_loss = F.mse_loss( output.local_context, output.remote_context.detach() )\n",
    "\n",
    "        # Apply the hidden dense layer to the context.\n",
    "        # remote_hidden.shape = [ batch_size, sequence_len, network_dim ]\n",
    "        output.remote_hidden = self.hidden_layer( output.remote_context )\n",
    "\n",
    "        # Project onto our target dimension.\n",
    "        # remote_hidden.shape = [ batch_size, sequence_len, vocab_size ]\n",
    "        output.remote_targets = self.target_layer( output.remote_hidden )\n",
    "\n",
    "        # Compute our loss against the remote context.\n",
    "        shift_targets = output.remote_targets[..., :-1, :].contiguous()\n",
    "        shift_inputs = inputs[..., 1:].contiguous()\n",
    "        output.remote_loss = self.loss_fct(shift_targets.view(-1, shift_targets.size(-1)), shift_inputs.view(-1))\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def filter_and_make_queries(self, output: SimpleNamespace, inputs:torch.LongTensor, n_to_query:int = 10) -> SimpleNamespace:\n",
    "        \"\"\" Filters peers based on activity and makes RPC queries.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pad the row weights to the network dimension.\n",
    "        self.row_weights = torch.nn.functional.pad(\n",
    "            self.row_weights, \n",
    "            pad = [0, self.metagraph.n() - self.row_weights.numel() ],\n",
    "            value = torch.mean(self.row_weights).item() # New values at the mean.\n",
    "        ).clone().detach().requires_grad_(True)\n",
    "       \n",
    "        # Get all neuron uids.\n",
    "        # all_uids = [n]\n",
    "        all_uids = metagraph.uids() \n",
    "        \n",
    "        # Filter uids based on last emit.\n",
    "        # filtered_uids = [ m <= n]\n",
    "        filtered_uids = all_uids[ torch.where( metagraph.block() - metagraph.lastemit() < 1000 ) ] \n",
    "\n",
    "        # Get corresponding weights.\n",
    "        # filtered_weights = [ m ]\n",
    "        filtered_weights = self.row_weights[ filtered_uids ]\n",
    "\n",
    "        # Get topk weights for filtered uids\n",
    "        # query_indices = [ n_to_query <= m ]\n",
    "        gamma = 0.9\n",
    "        output.query_weights, indices = torch.topk(\n",
    "            filtered_weights + torch.rand_like(filtered_weights) * gamma, \n",
    "            min(n_to_query, torch.numel(filtered_weights))\n",
    "        )\n",
    "        \n",
    "        # Duplicate inputs for each request.\n",
    "        # inputs_to_send = n_to_query * [ batch_size, sequence_length ]\n",
    "        output.query_uids = filtered_uids[ indices.tolist() ]\n",
    "        inputs_to_send = [ inputs for _ in output.query_uids.tolist() ]\n",
    "        neurons_to_query = [ metagraph.neurons()[ i ] for i in output.query_uids.tolist() ]\n",
    "        \n",
    "        # Make network calls. \n",
    "        # responses = n_to_query * [batch_size, sequence_length, network_dimension]\n",
    "        output.codes, responses = self.dendrite.forward_text( \n",
    "            neurons = neurons_to_query, \n",
    "            inputs = inputs_to_send\n",
    "        )\n",
    "        \n",
    "        # Weight-join responses.\n",
    "        # remote_context = [batch_size, sequence_length, network dimension]\n",
    "        stacked_responses = torch.stack( responses, dim=2 )\n",
    "        output.remote_context = torch.matmul( torch.transpose( stacked_responses, dim0=2, dim1=3), output.query_weights)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your nucleus\n",
    "model = GPT2Nucleus()\n",
    "model.metagraph = metagraph\n",
    "model.dendrite = dendrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test-Nucleus** <a class=\"anchor\" id=\"Test-Nucleus\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joined remote context from the network is: \n",
      "tensor([[[ 5.4727, -2.0498,  3.8389,  ..., -2.5904,  5.0177,  2.4396],\n",
      "         [ 0.9035, -2.1845,  2.1702,  ..., -5.0084,  6.2666, -0.6190]]],\n",
      "       grad_fn=<UnsafeViewBackward>)\n",
      "with shape: torch.Size([1, 2, 512])\n",
      "\n",
      "The distillation loss between your local and remote context is  67.44664001464844\n",
      "\n",
      "The loss with respect to your local context and the targets is 11.159619331359863\n",
      "\n",
      "The loss with respect to your remote context and the targets is 17.372888565063477\n",
      "\n",
      "You queried 50 remote neurons with uids\n",
      " tensor([ 88,  13, 220,  76, 186, 128, 193, 185, 341, 412, 227,  40, 372, 183,\n",
      "        184, 170,  28,  92, 334, 214,  12, 223, 371, 180,  53, 228, 222, 320,\n",
      "        215,   5,  51,  16, 204,  75, 206, 211,  62,  98,   9, 212, 205, 189,\n",
      "        380,  33, 379, 221, 229, 182,   2, 195])\n",
      "\n",
      "and response codes\n",
      " [0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Gradients w.r.t your row weights: \n",
      " [0.116, 0.204, -0.083, 0.28, 0.237, 0.0, -0.176, 0.269, 0.0, -0.098, 0.035, 0.641, 0.346, 0.052, -0.019, -0.327, 0.326, 0.459, 0.0, 0.153, 0.0, 0.233, 0.441, 0.056, -0.16, 0.024, 0.364, 0.0, 0.295, 0.04, 0.452, 0.678, -0.063, 0.0, -0.065, 0.179, 0.399, -0.28, -0.221, 0.047, -0.009, -0.084, 0.455, 0.291, 0.297, -0.061, 0.049, -0.09, 0.239, 0.052]\n"
     ]
    }
   ],
   "source": [
    "# Test remote forward call.\n",
    "inputs = torch.tensor([ \n",
    "    bittensor.__tokenizer__()('the cat', max_length=10, truncation=True)['input_ids'],  # Text sequence 1\n",
    "])\n",
    "output = model.remote_forward( inputs, n_to_query = 50 )\n",
    "loss = output.local_loss + output.remote_loss + output.distillation_loss\n",
    "loss.backward() #\n",
    "print ('The joined remote context from the network is: \\n{}\\nwith shape: {}\\n'.format(output.remote_context, output.remote_context.shape))\n",
    "print ('The distillation loss between your local and remote context is  {}\\n'.format(output.distillation_loss))\n",
    "print ('The loss with respect to your local context and the targets is {}\\n'.format(output.local_loss))\n",
    "print ('The loss with respect to your remote context and the targets is {}\\n'.format(output.remote_loss))\n",
    "print ('You queried {} remote neurons with uids\\n {}\\n\\nand response codes\\n {}\\n'.format(torch.numel(output.codes), output.query_uids, output.codes.tolist()))\n",
    "print ('Gradients w.r.t your row weights: \\n', [float('{:0.3f}'.format(model.row_weights.grad[idx].item())) for idx in output.query_uids.tolist()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training** <a class=\"anchor\" id=\"Training\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training-Loop** <a class=\"anchor\" id=\"Training-Loop\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import os\n",
    "\n",
    "# ---- Dataset ---- \n",
    "dataset = load_dataset('ag_news')['train']\n",
    "def nextbatch(data, batch_size, tokenizer):\n",
    "    \"\"\" Returns a random batch of sentences from text dataset.\n",
    "    \"\"\"\n",
    "    batch_text = []\n",
    "    for _ in range(batch_size):\n",
    "        batch_text.append(data[random.randint(0, len(data))]['text'])\n",
    "    batch_inputs = tokenizer(batch_text, return_tensors='pt', max_length=10, padding=True, truncation=True)['input_ids']\n",
    "    return batch_inputs\n",
    "\n",
    "# --- Training Logger ----\n",
    "if 'training_logger' not in locals():\n",
    "    training_log_dir = os.path.expanduser('~/logs/training.log')\n",
    "    logger.add(training_log_dir, filter=lambda record: record[\"extra\"].get(\"name\") == \"training\", enqueue=True, backtrace=True, diagnose=True, rotation=\"500 MB\")\n",
    "    training_logger = logger.bind(name=\"training\")\n",
    "def show_training_logs(length: int = 25):\n",
    "    ! tail -n $length $training_log_dir\n",
    "\n",
    "# ---- Tokenizer ----\n",
    "# For encoding text inputs.\n",
    "tokenizer = bittensor.__tokenizer__()\n",
    "\n",
    "# ---- Optimizer ----\n",
    "# For applying gradient steps to the local model.\n",
    "optimizer = torch.optim.SGD( model.parameters(), lr = 0.1, momentum = 0.99 )\n",
    "\n",
    "# ---- Training Loop -----\n",
    "def train( \n",
    "        stop_training: mp.Event,\n",
    "    ):\n",
    "    # ---- Loop until event is set ----\n",
    "    training_step = 0\n",
    "    batch_size = 1\n",
    "    logger.bind(training=True).info('Loop starting... ')\n",
    "    while not stop_training.is_set():\n",
    "        try:\n",
    "            optimizer.zero_grad() # Zeros out gradients for next accummulation\n",
    "\n",
    "            # ---- Forward pass ----\n",
    "            inputs = nextbatch( dataset, batch_size, tokenizer )\n",
    "            output = model.remote_forward( inputs, n_to_query=10 )\n",
    "\n",
    "            # ---- Backward pass ----\n",
    "            loss = output.local_loss + output.remote_loss + output.distillation_loss\n",
    "            loss.backward() # Accumulates gradients on the model.\n",
    "            clip_grad_norm_(model.parameters(), 0.8) # clip model gradients\n",
    "            optimizer.step() # Applies accumulated gradients.\n",
    "\n",
    "            # ---- Step logs ----\n",
    "            training_logger.info('->\\nuids:{}\\ncodes:{}\\nweights:{}\\ngrads:{}', \n",
    "                  output.query_uids, \n",
    "                  output.codes.tolist(), \n",
    "                  [float('{:0.3f}'.format(x)) for x in output.query_weights.tolist()],\n",
    "                  [float('{:0.3f}'.format(model.row_weights.grad[idx].item())) for idx in output.query_uids.tolist()])      \n",
    "            training_logger.info('gs:{} loss(local):{} loss(remote):{} loss(distill):{} dendrite:{}',\n",
    "                  colored('{}'.format( training_step ), 'red'),\n",
    "                  colored('{:.4f}'.format(output.local_loss.item()), 'green'),\n",
    "                  colored('{:.4f}'.format(output.remote_loss.item()), 'blue'),\n",
    "                  colored('{:.4f}'.format(output.distillation_loss.item()), 'red'),\n",
    "                  dendrite)\n",
    "\n",
    "            # --- Train and normalize row weights ---\n",
    "            model.row_weights = torch.clamp(model.row_weights - 0.001 * model.row_weights.grad, 0, 1)\n",
    "            model.row_weights = F.normalize( model.row_weights, p = 1, dim = 0 ).clone().detach().requires_grad_(True)\n",
    "\n",
    "            training_step += 1\n",
    "        except Exception as e:\n",
    "            training_logger.exception(\"Training iteration exception.\")\n",
    "    logger.bind(training=True).complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training Thread Runners** <a class=\"anchor\" id=\"Training-Thread-Runners\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import torch.multiprocessing as mp \n",
    "import sys\n",
    "\n",
    "join_timeout = 10\n",
    "\n",
    "if 'quit_training' in locals():\n",
    "    quit_training.set()\n",
    "if 'training_thread' in locals() and training_thread.is_alive():\n",
    "    training_thread.join( timeout = join_timeout )\n",
    "\n",
    "quit_training = mp.Event()\n",
    "training_thread = threading.Thread( target = train, args = (quit_training,),  name = 'training', daemon=True)\n",
    "\n",
    "def stop_training():\n",
    "    global quit_training\n",
    "    global training_thread\n",
    "    quit_training.set()\n",
    "    if not training_thread.is_alive():\n",
    "        return\n",
    "    training_logger.info(\"Joining...\")\n",
    "    training_thread.join( timeout = join_timeout )\n",
    "    if not training_thread.is_alive():\n",
    "        print ('Joined training thread',)\n",
    "        training_logger.info('Joined.')\n",
    "    else:\n",
    "        print ('Failed to join training thread')\n",
    "\n",
    "def start_training():\n",
    "    global quit_training\n",
    "    global training_thread\n",
    "    stop_training()\n",
    "    quit_training = mp.Event()\n",
    "    training_thread = threading.Thread( target = train, args = (quit_training,), name = 'training', daemon=True)\n",
    "    training_thread.start()\n",
    "    training_logger.info(\"Started training.\")\n",
    "    print('new training thread:', training_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (training_thread.is_alive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_training_logs(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Serving** <a class=\"anchor\" id=\"Serving\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'axon' in locals():\n",
    "    del axon\n",
    "axon = bittensor.Axon(\n",
    "    axon_local_ip = '127.0.0.1',\n",
    "    axon_local_port = 8082,\n",
    "    axon_external_port = 8082,\n",
    "    axon_external_ip = bittensor.utils.networking.get_external_ip(),\n",
    ")\n",
    "print (bittensor.Config.toString(axon.config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the axon serving endpoint.\n",
    "axon.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axon.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tail -n 10 '~/.bittensor/logs.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc \n",
    "channel = grpc.insecure_channel(\n",
    "            '127.0.0.1:8082',\n",
    "            options=[('grpc.max_send_message_length', -1),\n",
    "                     ('grpc.max_receive_message_length', -1)])\n",
    "stub = bittensor.grpc.BittensorStub( channel )\n",
    "\n",
    "\n",
    "inputs_raw = torch.tensor( [ [ 1 ] ], dtype=torch.int64)\n",
    "serializer = bittensor.serialization.get_serializer( serialzer_type = bittensor.proto.Serializer.MSGPACK )\n",
    "inputs_serialized = serializer.serialize(inputs_raw, modality = bittensor.proto.Modality.TEXT, from_type = bittensor.proto.TensorType.TORCH)\n",
    "request = bittensor.proto.TensorMessage(\n",
    "    version = bittensor.__version__,\n",
    "    public_key = 'sssss',\n",
    "    tensors = [inputs_serialized]\n",
    ")\n",
    "\n",
    "response = stub.Forward(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Serving-Loop** <a class=\"anchor\" id=\"Serving-Loop\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Serving logger ----\n",
    "if 'serving_logger' not in locals():\n",
    "    serving_log_dir = os.path.expanduser('~/logs/serving.log')\n",
    "    logger.add(serving_log_dir, colorize=True, filter=lambda record: record[\"extra\"].get(\"name\") == \"serving\", enqueue=True, backtrace=True, diagnose=True, rotation=\"500 MB\")\n",
    "    serving_logger = logger.bind(name=\"serving\")\n",
    "\n",
    "def show_serving_logs(length: int = 25):\n",
    "    ! tail -n $length $serving_log_dir\n",
    "\n",
    "# ---- Serving loop -----\n",
    "def serve ( \n",
    "  stop_serving: mp.Event,\n",
    "):\n",
    "\n",
    "    # ---- Loop until event is set -----\n",
    "    serving_step = 0\n",
    "    serving_logger.info('Serving thread started: ')\n",
    "    while not stop_serving.is_set():\n",
    "        try:\n",
    "            \n",
    "            # ---- Pull request ----\n",
    "            serving_logger.info('Axon:{}, waiting for query ... ', axon)\n",
    "            pong, pubkey, inputs, modality = axon.next_forward_item( timeout = 10.0 )\n",
    "\n",
    "            # ---- Process request ----\n",
    "            if None not in [ pong, pubkey, inputs, modality ]:\n",
    "                serving_logger.info('Recieved Query: from:{}, inputs.shape:{}', pubkey, inputs.shape)\n",
    "                output = model.local_forward( inputs ).local_hidden\n",
    "                pong.send( output.detach() )\n",
    "                serving_logger.info('Sent response: to:{}, output.shape:{}', pubkey, output.shape)\n",
    "\n",
    "        except Exception as e:\n",
    "            serving_logger.exception('Error in forward process with error {}', e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Serving Thread Runners** <a class=\"anchor\" id=\"Serving-Thread-Runners\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "join_timeout = 10\n",
    "\n",
    "if 'quit_serving' in locals():\n",
    "    quit_serving.set()\n",
    "if 'serving_thread' in locals() and serving_thread.is_alive():\n",
    "    serving_thread.join( timeout = join_timeout )\n",
    "\n",
    "quit_serving = mp.Event()\n",
    "serving_thread = threading.Thread( target = serve, args = (quit_serving,),  name = 'serving', daemon=True )\n",
    "\n",
    "def stop_serving():\n",
    "    global quit_serving\n",
    "    global serving_thread\n",
    "    quit_serving.set()\n",
    "    if serving_thread.is_alive():\n",
    "        serving_logger.info(\"Joining...\")\n",
    "        serving_thread.join( timeout = join_timeout )\n",
    "    if not serving_thread.is_alive():\n",
    "        print ('Joined serving thread',)\n",
    "        serving_logger.info('Joined.')\n",
    "    else:\n",
    "        print ('Failed to join serving thread')\n",
    "\n",
    "def start_serving():\n",
    "    global quit_serving\n",
    "    global serving_thread\n",
    "    stop_serving()\n",
    "    quit_serving = mp.Event()\n",
    "    serving_thread = threading.Thread( target = serve, args = (quit_serving,), name = 'serving', daemon=True )\n",
    "    serving_thread.start()\n",
    "    serving_logger.info(\"Started serving.\")\n",
    "    print('new serving thread:', serving_thread)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_serving()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (serving_thread.is_alive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_serving()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_serving_logs(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = bittensor.proto.Neuron(\n",
    "    address = axon.config.axon.local_ip,\n",
    "    port = axon.config.axon.local_port,\n",
    "    public_key = wallet.hotkey.public_key\n",
    ")\n",
    "start_time = time.time()\n",
    "codes, responses = dendrite.forward_text( \n",
    "    neurons = [ endpoint ],\n",
    "    inputs = [ torch.tensor([[1]]) ]\n",
    ")\n",
    "end_time = time.time()\n",
    "print(colored('Querying endpoint: {}:{}'.format(axon.config.axon.local_ip, axon.config.axon.local_port), 'blue'))\n",
    "if codes.item() == bittensor.proto.ReturnCode.Success:\n",
    "    print(colored('Success', 'green'))\n",
    "    print(colored('Response shape: {}'.format(responses[0].shape) , 'green'))\n",
    "    print(colored('Query time: {}'.format(end_time - start_time) , 'green'))\n",
    "else:\n",
    "    print(colored('Failure with code: {}'.format(codes.item()), 'red'))\n",
    "    print(colored('Ensure your axon is started with axon.start()', 'red'))\n",
    "    print(colored('Ensure your endpoint is accessible from the internet, perhaps behind your router\\'s NAT?', 'red'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Filtering Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the trained weights from the chain.\n",
    "weights_to_emit = torch.nn.functional.pad(\n",
    "    model.row_weights, \n",
    "    pad = [0, metagraph.n() - model.row_weights.numel() ],\n",
    "    value = torch.mean(model.row_weights).item()\n",
    ")\n",
    "\n",
    "model.row_weights.detach()\n",
    "\n",
    "# Take topk\n",
    "topk = 30\n",
    "weights_to_emit, uids = torch.topk(weights_to_emit, topk)\n",
    "\n",
    "# Normalize to 0,1\n",
    "weights_to_emit = F.normalize(weights_to_emit, p = 1, dim = 0)\n",
    "print (\"Weights:\\n{}\\nFor uids\\n {}\".format(weights_to_emit.tolist(), uids.tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets your incentive weights on the chain.\n",
    "subtensor.set_weights(\n",
    "    uids = uids,\n",
    "    weights = weights_to_emit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_uid = bittensor.metagraph.uids()[0]\n",
    "print ('Your weights (encoded as uint32s) on the chain are: \\n\\n {} \\n'.format(bittensor.subtensor.weight_vals_for_uid( your_uid )))\n",
    "\n",
    "print ('For uids \\n {}'.format(bittensor.subtensor.weight_uids_for_uid( your_uid )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transactions** <a class=\"anchor\" id=\"Transactions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = bittensor.Executor(\n",
    "    wallet = wallet,\n",
    "    subtensor = subtensor,\n",
    "    metagraph = metagraph\n",
    ")\n",
    "bittensor.BITTENSOR_STDOUT_LOGGING_LEVEL = 'SUCCESS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSubtensor connected to: kusanagi\u001b[0m\n",
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "\n",
      "\u001b[34mRetrieving all nodes associated with coldkey: 0x3c9cd1679888e5660b0c8e4b8a17a1719c0cb7f05b5c624a856b421b52290515\u001b[0m\n",
      "BALANCE: 5DSBKDdQm6DA1BWYXoyZD4ta1HKAws2raVrfzjg22cy4QRXb : [τ9.599999436]\n",
      "\n",
      "--===[[ STAKES ]]===--\n",
      "+-----+----------------+----------------+----------------+-------------------------+\n",
      "| UID | IP             | STAKE (τ)      | RANK (τ)       | INCENTIVE (τ)           |\n",
      "+-----+----------------+----------------+----------------+-------------------------+\n",
      "| 168 | 181.176.116.47 | τ245.040477431 | τ119.553343488 | τ0.00016468367539346218 |\n",
      "+-----+----------------+----------------+----------------+-------------------------+\n",
      "Total stake:  τ245.040477431\n"
     ]
    }
   ],
   "source": [
    "executor.overview()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Unstaking-Funds** <a class=\"anchor\" id=\"Unstaking-Funds\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSubtensor connected to: kusanagi\u001b[0m\n",
      "\u001b[34mRetrieving all nodes associated with coldkey: 0x3c9cd1679888e5660b0c8e4b8a17a1719c0cb7f05b5c624a856b421b52290515\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:26.778\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36msendMessage\u001b[0m:\u001b[36m395\u001b[0m - \u001b[34m\u001b[1mSending message: b'{\"jsonrpc\": \"2.0\", \"method\": \"chain_getRuntimeVersion\", \"params\": [null], \"id\": 2797}'\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:26.921\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m352\u001b[0m - \u001b[34m\u001b[1mrecieved message with id 2797\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:26.925\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36msendMessage\u001b[0m:\u001b[36m395\u001b[0m - \u001b[34m\u001b[1mSending message: b'{\"jsonrpc\": \"2.0\", \"method\": \"state_getPairs\", \"params\": [\"0x658faa385070e074c85bf6b568cf0555acdb62a15501cc6a0710e5324f100a9e\", null], \"id\": 2798}'\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:27.652\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m352\u001b[0m - \u001b[34m\u001b[1mrecieved message with id 2798\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:27.716\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36msendMessage\u001b[0m:\u001b[36m395\u001b[0m - \u001b[34m\u001b[1mSending message: b'{\"jsonrpc\": \"2.0\", \"method\": \"chain_getRuntimeVersion\", \"params\": [null], \"id\": 2799}'\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:27.836\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m352\u001b[0m - \u001b[34m\u001b[1mrecieved message with id 2799\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:27.840\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36msendMessage\u001b[0m:\u001b[36m395\u001b[0m - \u001b[34m\u001b[1mSending message: b'{\"jsonrpc\": \"2.0\", \"method\": \"state_getStorageAt\", \"params\": [\"0x658faa385070e074c85bf6b568cf055522fbe0bd0cb77b6b6f365f641b0de381a800000000000000\", null], \"id\": 2800}'\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:27.979\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m352\u001b[0m - \u001b[34m\u001b[1mrecieved message with id 2800\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:27.982\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36msendMessage\u001b[0m:\u001b[36m395\u001b[0m - \u001b[34m\u001b[1mSending message: b'{\"jsonrpc\": \"2.0\", \"method\": \"chain_getRuntimeVersion\", \"params\": [null], \"id\": 2801}'\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:28.109\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m352\u001b[0m - \u001b[34m\u001b[1mrecieved message with id 2801\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:28.111\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36msendMessage\u001b[0m:\u001b[36m395\u001b[0m - \u001b[34m\u001b[1mSending message: b'{\"jsonrpc\": \"2.0\", \"method\": \"state_getStorageAt\", \"params\": [\"0x658faa385070e074c85bf6b568cf055522fbe0bd0cb77b6b6f365f641b0de381a800000000000000\", null], \"id\": 2802}'\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:28.252\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m352\u001b[0m - \u001b[34m\u001b[1mrecieved message with id 2802\u001b[0m\n",
      "\u001b[34mRequesting unstake of 10000000000 rao for hotkey: 0x80cacfbdf7b155b39de22680a7cb14c61a8f95df702c92f5f142d25cca37c545 to coldkey: 0x3c9cd1679888e5660b0c8e4b8a17a1719c0cb7f05b5c624a856b421b52290515\u001b[0m\n",
      "\u001b[37mWaiting for finalization...\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:28.259\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36msendMessage\u001b[0m:\u001b[36m395\u001b[0m - \u001b[34m\u001b[1mSending message: b'{\"jsonrpc\": \"2.0\", \"method\": \"chain_getRuntimeVersion\", \"params\": [null], \"id\": 2803}'\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:28.375\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m352\u001b[0m - \u001b[34m\u001b[1mrecieved message with id 2803\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:28.377\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36msendMessage\u001b[0m:\u001b[36m395\u001b[0m - \u001b[34m\u001b[1mSending message: b'{\"jsonrpc\": \"2.0\", \"method\": \"chain_getBlockHash\", \"params\": [0], \"id\": 2804}'\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:28.528\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m352\u001b[0m - \u001b[34m\u001b[1mrecieved message with id 2804\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:28.531\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36msendMessage\u001b[0m:\u001b[36m395\u001b[0m - \u001b[34m\u001b[1mSending message: b'{\"jsonrpc\": \"2.0\", \"method\": \"author_submitAndWatchExtrinsic\", \"params\": [\"0x3d0284f8b3da5792a2cc7e05857ae4703aee67133944701ecfa806deefc6516a883560016ae99d9595a86876bbd29101d2752cf88864432358cc1cbd516069a38b0a02294f47b92efaca0d96a489d90443e5ebbcac979e08cc41e986920799c313517882002400080280cacfbdf7b155b39de22680a7cb14c61a8f95df702c92f5f142d25cca37c54500e40b5402000000\"], \"id\": 2805}'\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:28.733\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m352\u001b[0m - \u001b[34m\u001b[1mrecieved message with id 2805\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:28.734\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m376\u001b[0m - \u001b[34m\u001b[1mhandler produced non-result\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:28.735\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m352\u001b[0m - \u001b[34m\u001b[1mrecieved message with id 2805\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:28.735\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m376\u001b[0m - \u001b[34m\u001b[1mhandler produced non-result\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:28.882\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m352\u001b[0m - \u001b[34m\u001b[1mrecieved message with id 2805\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:28.882\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m376\u001b[0m - \u001b[34m\u001b[1mhandler produced non-result\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:30.767\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m352\u001b[0m - \u001b[34m\u001b[1mrecieved message with id 2805\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:30.768\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m376\u001b[0m - \u001b[34m\u001b[1mhandler produced non-result\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:45.325\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m352\u001b[0m - \u001b[34m\u001b[1mrecieved message with id 2805\u001b[0m\n",
      "\u001b[32m2021-04-08 17:19:45.326\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36mresult_handler\u001b[0m:\u001b[36m1387\u001b[0m - \u001b[34m\u001b[1mrequest is finalized\u001b[0m\n",
      "\u001b[32mUnstaked:10.0 from uid:168 to coldkey.pub:0x3c9cd1679888e5660b0c8e4b8a17a1719c0cb7f05b5c624a856b421b52290515\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "executor.unstake( amount_tao = 10, uid = 168 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Staking-Funds** <a class=\"anchor\" id=\"Staking-Funds\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSubtensor connected to: kusanagi\u001b[0m\n",
      "\u001b[32m2021-04-08 17:17:32.365\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36msendMessage\u001b[0m:\u001b[36m395\u001b[0m - \u001b[34m\u001b[1mSending message: b'{\"jsonrpc\": \"2.0\", \"method\": \"chain_getRuntimeVersion\", \"params\": [null], \"id\": 2795}'\u001b[0m\n",
      "\u001b[32m2021-04-08 17:17:32.506\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m352\u001b[0m - \u001b[34m\u001b[1mrecieved message with id 2795\u001b[0m\n",
      "\u001b[32m2021-04-08 17:17:32.510\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36msendMessage\u001b[0m:\u001b[36m395\u001b[0m - \u001b[34m\u001b[1mSending message: b'{\"jsonrpc\": \"2.0\", \"method\": \"state_getStorageAt\", \"params\": [\"0x26aa394eea5630e07c48ae0c9558cef7b99d880ec681799c0cf30e8886371da99beb43fa45b94c3706b5d74d446fbf873c9cd1679888e5660b0c8e4b8a17a1719c0cb7f05b5c624a856b421b52290515\", null], \"id\": 2796}'\u001b[0m\n",
      "\u001b[32m2021-04-08 17:17:32.634\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mbittensor.substrate\u001b[0m:\u001b[36monMessage\u001b[0m:\u001b[36m352\u001b[0m - \u001b[34m\u001b[1mrecieved message with id 2796\u001b[0m\n",
      "\u001b[31mNot enough balance (Tao 9.599999436) to stake Tao 10.000000000\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'quit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-15ab52547f2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstake\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mamount_tao\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m168\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Workspace/bittensor/bittensor/executor.py\u001b[0m in \u001b[0;36mstake\u001b[0;34m(self, amount_tao, uid)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbalance\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mamount_balance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mbittensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__user_logger__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Not enough balance ({}) to stake {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbalance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamount_balance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mneurons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_associated_neurons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'quit' is not defined"
     ]
    }
   ],
   "source": [
    "executor.stake( amount_tao = 10, uid = 168 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tail -n 10 '~/.bittensor/logs.log'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transfering-Funds** <a class=\"anchor\" id=\"Transfering-Funds\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = 0.01\n",
    "destination_public_key = wallet.coldkey.public_key\n",
    "amount = Balance.from_float( amount )\n",
    "balance = bittensor.subtensor.get_balance( wallet.coldkey.public_key )\n",
    "if balance < amount:\n",
    "    print(colored(\"Not enough balance \\u03C4{} to transfer \\u03C4{}\".format(balance, amount), 'red'))\n",
    "    quit()\n",
    "\n",
    "print(colored(\"Requesting transfer of \\u03C4{}, from coldkey.pub: {} to dest.pub: {}\".format(amount.tao, wallet.coldkey.public_key, destination_public_key), 'blue'))\n",
    "print(\"Waiting for finalization...\",)\n",
    "result = bittensor.subtensor.transfer(destination_public_key, amount, wait_for_finalization = True, timeout = bittensor.__blocktime__ * 5)\n",
    "if result:\n",
    "    print(colored(\"Transfer finalized with amount: \\u03C4{} to dest: {} from coldkey.pub: {}\".format(amount.tao, destination_public_key, wallet.coldkey.public_key), 'green'))\n",
    "    new_balance = bittensor.subtensor.get_balance(wallet.coldkeypub)\n",
    "    destination_balance = bittensor.subtensor.get_balance(destination_public_key)\n",
    "    print(colored(\"Your coldkey has new balance: \\u03C4{}\".format( new_balance.tao ) , 'green'))\n",
    "    print(colored(\"The destination has new balance: \\u03C4{}\".format( new_balance.tao ) , 'green'))\n",
    "else:\n",
    "    print(colored(\"Transfer failed\", 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
