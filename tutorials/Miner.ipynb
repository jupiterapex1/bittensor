{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloads the code base\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloads the code base\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bittensor\n",
    "import os\n",
    "import torch\n",
    "import torch.multiprocessing as mp \n",
    "import time\n",
    "from loguru import logger\n",
    "from termcolor import colored\n",
    "import nest_asyncio \n",
    "nest_asyncio.apply()\n",
    "logger.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Init Bittensor** <a class=\"anchor\" id=\"Init-Bittensor\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Wallet** <a class=\"anchor\" id=\"Wallet\"></a>\n",
    "\n",
    "<span style=\"color:green\">**Description:**</span> The following cell creates your bittensor wallet. Your wallet holds two keys for you:\n",
    "\n",
    "* <span style=\"color:blue\">***coldkey***</span> (~/.bittensor/wallets/default/YOUR_WALLET_NAME):\n",
    "    * Your coldkey is used to store, transfer, and stake tokens. It is \"cold\" because it is not loaded into the miner and remains encrypted on the device. \n",
    "<br />\n",
    "<br />\n",
    "\n",
    "* <span style=\"color:blue\">***hotkey***</span> (~/.bittensor/wallets/YOUR_WALLET_NAME/hotkeys/YOUR_HOTKEY_NAME):  \n",
    "    * Your hotkey is used by the miner to subscribe and set weights. It is \"hot\" because it is loaded into the running software (which can be insecure). It does not have permission to move funds.\n",
    "\n",
    "\n",
    "<span style=\"color:red\">**IMPORTANT**</span>\n",
    "\n",
    "If you create keys ensure you store the generated mnemonic for **both** your hot and coldkey \n",
    "you will need these to recover your keys if you forget your password or lose access to this machine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in below to name your wallet and hotkey.\n",
    "YOUR_WALLET_NAME = 'colab'\n",
    "YOUR_HOTKEY_NAME = 'colab_hot'\n",
    "\n",
    "# Fill in below if your need to regenerate your keys.\n",
    "use_mnemonic = False # Set to true for key regeneration.\n",
    "coldkey_mnemonic = \"<to be filled>\".split(' ')\n",
    "hotkey_mnemonic = \"<to be filled>\".split(' ')\n",
    "\n",
    "# Create the wallet object.\n",
    "wallet = bittensor.Wallet(\n",
    "    path = \"~/.bittensor/wallets/\",\n",
    "    name = YOUR_WALLET_NAME,\n",
    "    hotkey = YOUR_HOTKEY_NAME\n",
    ")\n",
    "\n",
    "# Optionally regens/creates your wallet keys.\n",
    "if not wallet.has_coldkeypub:\n",
    "    if use_mnemonic:\n",
    "        wallet.regenerate_coldkey(mnemonic = coldkey_mnemonic, use_password = True)\n",
    "    else:\n",
    "        wallet.create_new_coldkey(n_words = 12, use_password = True )\n",
    "if not wallet.has_hotkey:\n",
    "    if use_mnemonic:\n",
    "        wallet.regenerate_hotkey(mnemonic = hotkey_mnemonic)\n",
    "    else:\n",
    "        wallet.create_new_hotkey(n_words = 12)\n",
    "\n",
    "# Assert before continuing\n",
    "assert wallet.has_hotkey\n",
    "assert wallet.has_coldkeypub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Config** <a class=\"anchor\" id=\"Config\"></a>\n",
    "\n",
    "<span style=\"color:green\">**Description:**</span> The following cells instantiate the Bittensor's background components through a config object. These are:\n",
    "\n",
    "* <span style=\"color:blue\">***subtensor***</span> (~/.bittensor/bittensor/subtensor.py):\n",
    "    * A websocket connection to the blockchain; used to query chain state and make transactions.\n",
    "<br />\n",
    "* <span style=\"color:blue\">***metagraph***</span> (~/.bittensor/bittensor/subtensor.py):\n",
    "    * An object which maintains synced chain state as torch tensors.\n",
    "<br />\n",
    "* <span style=\"color:blue\">***axon***</span> (~/.bittensor/bittensor/axon.py):\n",
    "    * An RPC server which accepts requests from other peers in the network and makes those requests available to your miner.\n",
    "<br />\n",
    "* <span style=\"color:blue\">***dendrite***</span> (~/.bittensor/bittensor/dendrite.py):\n",
    "    * An RPC client which maintains an interface to other peers as a queryable, autograd-friendly torch.nn.Module.\n",
    "\n",
    "<span style=\"color:red\">**IMPORTANT:**</span> If bittensor is already initialized running these commands will recreate these components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to print all config items: \n",
    "# bittensor.help()\n",
    "config = bittensor.Neuron.default_config()\n",
    "config.neuron.multiprocessing = False\n",
    "config.neuron.debug = True\n",
    "config.receptor.do_backoff = False\n",
    "config.receptor.timeout = 2\n",
    "config.subtensor.network = 'kusanagi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init bittensor components\n",
    "# NOTE: Running this call recreates bittensor objects.\n",
    "bittensor.init( with_config = config, with_wallet = wallet )\n",
    "print ( bittensor.Config.toString( bittensor.config ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Subscription** <a class=\"anchor\" id=\"Subscription\"></a>\n",
    "\n",
    "<span style=\"color:green\">**Description:**</span> Peers on the network advertise their existence by subscribing an RPC endpoint to the blockchain. <br />\n",
    "Run the following cell to subscribe your endpoint; telling the others which ip/port you will be <br />\n",
    "recieving queries on.\n",
    "\n",
    "<span style=\"color:red\">**IMPORTANT:**</span> Ensure you re-run this command if your endpoint changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribe our endpoint to the chain.\n",
    "bittensor.subtensor.subscribe(\n",
    "    config.axon.external_ip, \n",
    "    config.axon.external_port,\n",
    "    config.neuron.modality,\n",
    "    wallet.coldkeypub,\n",
    "    wait_for_finalization = True,\n",
    "    timeout = 4 * bittensor.__blocktime__,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Metagraph** <a class=\"anchor\" id=\"Metagraph\"></a> \n",
    "\n",
    "<span style=\"color:green\">**Description:**</span> The following cell syncs the latest chain-state into your metagraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syncs the latest chain information into the metagraph\n",
    "# NOTE: this command is expensive.\n",
    "bittensor.metagraph.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_block =  bittensor.metagraph.block()\n",
    "uids_on_chain = bittensor.metagraph.uids()\n",
    "n_neurons = torch.max(bittensor.metagraph.uids())\n",
    "n_online = torch.numel(\n",
    "  bittensor.metagraph.uids()[ \n",
    "    torch.where(\n",
    "      bittensor.metagraph.block() - bittensor.metagraph.lastemit() < 100\n",
    "    )\n",
    "  ]\n",
    ")\n",
    "print ('The chain is at block: {}\\n'.format(bittensor.metagraph.block()))\n",
    "print ('There are {} neurons on the network\\n'.format(n_neurons))\n",
    "print ('These are their uids: {}\\n'.format(bittensor.metagraph.uids()))\n",
    "print ('{} neurons have emitted in the last 100 blocks\\n'.format(n_online))\n",
    "print ('{} Tao is staked'.format(torch.sum(bittensor.metagraph.S())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = bittensor.metagraph.uid_for_pubkey(wallet.hotkey.public_key)\n",
    "index = bittensor.metagraph.index_for_uid(uid)\n",
    "stake = bittensor.metagraph.S()[index]\n",
    "rank = bittensor.metagraph.R()[index]\n",
    "incentive = bittensor.metagraph.I()[index]\n",
    "neuron = bittensor.metagraph.neurons()[index]\n",
    "position = (torch.argsort(bittensor.metagraph.R(), dim=0) == index).nonzero(as_tuple=True)[0].item()\n",
    "wallet_balance = bittensor.subtensor.get_balance(wallet.coldkeypub)\n",
    "print ('Your network uid is {} for hotkey public key {}\\n'.format(uid, wallet.hotkey.public_key))\n",
    "print ('Your are subscribed at endpoint {}:{} for modality {}\\n'.format(neuron.address, neuron.port, neuron.modality))\n",
    "print ('You are staking \\u03C4{} \\n\\nYou have a network rank of \\u03C4{} for position {}/{}\\n\\nand are attaining incentive: \\u03C4{}/block'.format(stake, rank, position, n_neurons, incentive))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Nucleus**  <a class=\"anchor\" id=\"Nucleus\"></a>\n",
    "\n",
    "<span style=\"color:green\">**Description:**</span> The following cell contains a bittensor ***Nucleus***: a custom machine learning model which can be served on the network. <br />\n",
    "The model below uses a GPT2 kernel and learns from the AG-News dataset. ***Feel free to hack this part of the code.***\n",
    "\n",
    "<span style=\"color:red\">**IMPORTANT:**</span> Save this model between runs in-case your miner crashes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GPT2-Nucleus** <a class=\"anchor\" id=\"GPT2-Nucleus\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from types import SimpleNamespace\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "class GPT2Nucleus(torch.nn.Module):\n",
    "    # A simple as it gets Bittensor nucleus using a GPT2 kernel\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        huggingface_config = GPT2Config( vocab_size=bittensor.__vocab_size__, n_embd=bittensor.__network_dim__, n_layer=2, n_head=1, n_inner=8 )\n",
    "        self.transformer = GPT2Model(huggingface_config)\n",
    "        self.hidden_layer = nn.Linear( bittensor.__network_dim__, bittensor.__network_dim__ )\n",
    "        self.target_layer = nn.Linear( bittensor.__network_dim__, bittensor.__vocab_size__, bias=False )\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # The scores you learn for other neurons in the network.\n",
    "        # NOTE: This needs to be updated every time you re-sync your metagraph.\n",
    "        self.row_weights = torch.rand( bittensor.metagraph.n(), requires_grad=True)\n",
    "\n",
    "    def local_forward(self, inputs: torch.LongTensor):\n",
    "        # Runs only the local part of the model.\n",
    "\n",
    "        # To be filled.\n",
    "        output = SimpleNamespace()\n",
    "\n",
    "        # Apply our GPT transformer model.\n",
    "        # local_context.shape = [ batch_size, sequence_len, network_dim ]\n",
    "        output.local_context = self.transformer(input_ids=inputs, return_dict=True).last_hidden_state\n",
    "\n",
    "        # Apply our dense layer and project it onto our target layer.\n",
    "        # local_hidden.shape = [ batch_size, sequence_len, network_dim ]\n",
    "        output.local_hidden = self.hidden_layer( output.local_context )\n",
    "\n",
    "        # Project to our target dimension.\n",
    "        # local_targets.shape = [ batch_size, sequence_len, vocab_size ]\n",
    "        output.local_targets = self.target_layer( output.local_hidden )\n",
    "\n",
    "        # Compute LM-loss \n",
    "        shift_targets = output.local_targets[..., :-1, :].contiguous()\n",
    "        shift_inputs = inputs[..., 1:].contiguous()\n",
    "        output.local_loss = self.loss_fct(shift_targets.view(-1, shift_targets.size(-1)), shift_inputs.view(-1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def remote_forward(self, inputs: torch.LongTensor, n_to_query:int = 10):\n",
    "        # Runs the model with calls on the network.\n",
    "\n",
    "        # Run the local model.\n",
    "        output = self.local_forward( inputs )\n",
    "\n",
    "        # Select peers to query based on weights + random pertubation.\n",
    "        gamma = 0.9\n",
    "        output.query_weights, output.query_indices = torch.topk(\n",
    "            self.row_weights + torch.rand_like(self.row_weights) * gamma, \n",
    "            n_to_query\n",
    "        )\n",
    "        neurons_to_call = [ bittensor.metagraph.neurons()[idx] for idx in output.query_indices.tolist() ]\n",
    "        inputs_to_send = [ inputs for _ in output.query_indices.tolist() ]\n",
    "        output.query_uids = [bittensor.metagraph.uid_for_pubkey(neuron.public_key) for neuron in neurons_to_call]\n",
    "\n",
    "        # Make network calls and then weight-join the responses.\n",
    "        output.codes, responses = bittensor.forward_text( \n",
    "            neurons = neurons_to_call, \n",
    "            inputs = inputs_to_send\n",
    "        )\n",
    "        stacked_responses = torch.stack( responses, dim=2 )\n",
    "        output.remote_context = torch.matmul( torch.transpose( stacked_responses, dim0=2, dim1=3), output.query_weights)\n",
    "\n",
    "        # Compute the distillation loss between the local and remote context\n",
    "        output.distillation_loss = F.mse_loss( output.local_context, output.remote_context.detach() )\n",
    "\n",
    "        # Apply the hidden dense layer to the context.\n",
    "        output.remote_hidden = self.hidden_layer( output.remote_context )\n",
    "\n",
    "        # Project to our target dimension.\n",
    "        output.remote_targets = self.target_layer( output.remote_hidden )\n",
    "\n",
    "        # Compute our loss against our remote context.\n",
    "        shift_targets = output.remote_targets[..., :-1, :].contiguous()\n",
    "        shift_inputs = inputs[..., 1:].contiguous()\n",
    "        output.remote_loss = self.loss_fct(shift_targets.view(-1, shift_targets.size(-1)), shift_inputs.view(-1))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your nucleus\n",
    "model = GPT2Nucleus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test-Nucleus** <a class=\"anchor\" id=\"Test-Nucleus\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test remote forward call.\n",
    "inputs = torch.tensor([bittensor.__tokenizer__()('the cat')['input_ids']])\n",
    "output = model.remote_forward( inputs, n_to_query=20)\n",
    "loss = output.local_loss + output.remote_loss + output.distillation_loss\n",
    "loss.backward() #\n",
    "print ('The distillation loss between your local and remote context is  {}\\n'.format(output.distillation_loss))\n",
    "print ('The loss with respect to your local context and the targets is {}\\n'.format(output.local_loss))\n",
    "print ('The loss with respect to your remote context and the targets is {}\\n'.format(output.remote_loss))\n",
    "print ('You queried {} remote neurons with\\n\\nuids\\n {}\\n\\nresponse codes\\n {}\\n'.format(torch.numel(output.codes), output.query_uids, output.codes.tolist()))\n",
    "print ('and gradients w.r.t your row weights\\n', [float('{:0.3f}'.format(model.row_weights.grad[idx].item())) for idx in output.query_indices.tolist()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row-Weights\n",
    "\n",
    "This model uses a parameter **row_weights** to select which peers to query during each call of model.remote_forward. <br>\n",
    "The row_weights **are trainable with respect to the loss** because they are used to join these resonses like a typical Sparsely Gated Layer.\n",
    "The miner uses these trained weights as proxy for the scores it sinks to the chain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training** <a class=\"anchor\" id=\"Training\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training-Loop** <a class=\"anchor\" id=\"Training-Loop\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import os\n",
    "\n",
    "# ---- Dataset ---- \n",
    "dataset = load_dataset('ag_news')['train']\n",
    "def nextbatch(data, batch_size, tokenizer):\n",
    "    \"\"\" Returns a random batch of sentences from text dataset.\n",
    "    \"\"\"\n",
    "    batch_text = []\n",
    "    for _ in range(batch_size):\n",
    "        batch_text.append(data[random.randint(0, len(data))]['text'])\n",
    "    batch_inputs = tokenizer(batch_text, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "    return batch_inputs\n",
    "\n",
    "# --- Training Logger ----\n",
    "logger.remove()\n",
    "training_log_dir = os.path.expanduser('~/logs/training.log')\n",
    "logger.add(training_log_dir, filter=lambda record: \"training\" in record[\"extra\"], enqueue=True, backtrace=True, diagnose=True, rotation=\"500 MB\")\n",
    "def show_training_logs(length: int = 25):\n",
    "    ! tail -n $length $training_log_dir\n",
    "\n",
    "# ---- Tokenizer ----\n",
    "# For encoding text inputs.\n",
    "tokenizer = bittensor.__tokenizer__()\n",
    "\n",
    "# ---- Optimizer ----\n",
    "# For applying your gradient steps to the local model.\n",
    "optimizer = torch.optim.SGD( model.parameters(), lr = 0.1, momentum = 0.99 )\n",
    "\n",
    "# ---- Training Loop -----\n",
    "def train( \n",
    "        stop_training: mp.Event,\n",
    "    ):\n",
    "    # ---- Loop until event is set ----\n",
    "    training_step = 0\n",
    "    batch_size = 5\n",
    "    logger.bind(training=True).info('Loop starting... ')\n",
    "    while not stop_training.is_set():\n",
    "        try:\n",
    "            optimizer.zero_grad() # Zeros out gradients for next accummulation\n",
    "\n",
    "            # ---- Forward pass ----\n",
    "            inputs = nextbatch( dataset, batch_size, tokenizer )\n",
    "            outputs = model.remote_forward( inputs )\n",
    "\n",
    "            # ---- Backward pass ----\n",
    "            loss = outputs.local_loss + outputs.remote_loss + outputs.distillation_loss\n",
    "            loss.backward() # Accumulates gradients on the model.\n",
    "            clip_grad_norm_(model.parameters(), 0.8) # clip model gradients\n",
    "            optimizer.step() # Applies accumulated gradients.\n",
    "\n",
    "            # ---- Step logs ----\n",
    "            logger.bind(training=True).info('->\\nuids:{}\\ncodes:{}\\nweights:{}\\ngrads:{}', \n",
    "                  outputs.query_uids, \n",
    "                  outputs.codes.tolist(), \n",
    "                  [float('{:0.3f}'.format(x)) for x in outputs.query_weights.tolist()],\n",
    "                  [float('{:0.3f}'.format(model.row_weights.grad[idx].item())) for idx in outputs.query_indices.tolist()])      \n",
    "            logger.bind(training=True).info('gs:{} loss(local):{} loss(remote):{} loss(distill):{} dendrite:{}',\n",
    "                  colored('{}'.format( training_step ), 'red'),\n",
    "                  colored('{:.4f}'.format(outputs.local_loss.item()), 'green'),\n",
    "                  colored('{:.4f}'.format(outputs.remote_loss.item()), 'blue'),\n",
    "                  colored('{:.4f}'.format(outputs.distillation_loss.item()), 'red'),\n",
    "                  bittensor.neuron.dendrite.toString())\n",
    "            training_step += 1\n",
    "        except Exception as e:\n",
    "            logger.bind(training=True).exception(\"Training iteration exception.\")\n",
    "    logger.bind(training=True).complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training Thread Runners** <a class=\"anchor\" id=\"Training-Thread-Runners\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import torch.multiprocessing as mp \n",
    "import sys\n",
    "\n",
    "join_timeout = 10\n",
    "\n",
    "if 'quit_training' in locals():\n",
    "    quit_training.set()\n",
    "if 'training_thread' in locals() and training_thread.is_alive():\n",
    "    training_thread.join( timeout = join_timeout )\n",
    "\n",
    "quit_training = mp.Event()\n",
    "training_thread = threading.Thread( target = train, args = (quit_training,),  name = 'training', daemon=True)\n",
    "\n",
    "def stop_training():\n",
    "    global quit_training\n",
    "    global training_thread\n",
    "    quit_training.set()\n",
    "    if not training_thread.is_alive():\n",
    "        return\n",
    "    logger.bind(training=True).info(\"Joining...\")\n",
    "    training_thread.join( timeout = join_timeout )\n",
    "    if not training_thread.is_alive():\n",
    "        print ('Joined training thread',)\n",
    "        logger.bind(training=True).info('Joined.')\n",
    "    else:\n",
    "        print ('Failed to join training thread')\n",
    "\n",
    "def start_training():\n",
    "    global quit_training\n",
    "    global training_thread\n",
    "    stop_training()\n",
    "    quit_training = mp.Event()\n",
    "    training_thread = threading.Thread( target = train, args = (quit_training,), name = 'training', daemon=True)\n",
    "    training_thread.start()\n",
    "    logger.bind(training=True).info(\"Started training.\")\n",
    "    print('new training thread:', training_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (training_thread.is_alive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_training_logs(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Serving** <a class=\"anchor\" id=\"Serving\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the axon serving endpoint.\n",
    "bittensor.axon.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Serving-Loop** <a class=\"anchor\" id=\"Serving-Loop\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Serving logger ----\n",
    "serving_log_dir = os.path.expanduser('~/logs/serving.log')\n",
    "logger.add(serving_log_dir, filter=lambda record: \"serving\" in record[\"extra\"], enqueue=True, backtrace=True, diagnose=True, rotation=\"500 MB\")\n",
    "def show_serving_logs(length: int = 25):\n",
    "    ! tail -n $length $serving_log_dir\n",
    "\n",
    "# ---- Serving loop -----\n",
    "def serve ( \n",
    "  stop_serving: mp.Event,\n",
    "):\n",
    "\n",
    "    # ---- Loop until event is set -----\n",
    "    serving_step = 0\n",
    "    logger.bind(serving=True).info('Serving thread started: ')\n",
    "    while not stop_serving.is_set():\n",
    "\n",
    "        # ---- Pull request ----\n",
    "        logger.bind(serving=True).info('Axon:{}, waiting for query ... ', bittensor.axon.toString())\n",
    "        pong, pubkey, inputs, modality = bittensor.axon.next_forward_item( timeout = 10.0 )\n",
    "\n",
    "        # ---- Process request ----\n",
    "        if None not in [ pong, pubkey, inputs, modality]:\n",
    "            logger.bind(serving=True).info('Recieved Query: from:{}, inputs.shape:{}', pubkey, inputs.shape)\n",
    "            try:          \n",
    "                outputs = model.local_forward( inputs ).local_hidden\n",
    "                pong.send( outputs.detach() )\n",
    "                logger.bind(serving=True).info('Sent response: to:{}, outputs.shape:{}', pubkey, outputs.shape)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.bind(serving=True).exception('Error in forward process with error {}', e)\n",
    "                continue\n",
    "\n",
    "      # ---- Tensorboard ----\n",
    "      #bittensor.neuron.axon.toTensorboard(serving_tensorboard, serving_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Serving Thread Runners** <a class=\"anchor\" id=\"Serving-Thread-Runners\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_timeout = 10\n",
    "\n",
    "if 'quit_serving' in locals():\n",
    "    quit_serving.set()\n",
    "if 'serving_thread' in locals() and serving_thread.is_alive():\n",
    "    serving_thread.join( timeout = join_timeout )\n",
    "\n",
    "quit_serving = mp.Event()\n",
    "serving_thread = threading.Thread( target = serve, args = (quit_serving,),  name = 'serving', daemon=True)\n",
    "\n",
    "def stop_serving():\n",
    "    global quit_serving\n",
    "    global serving_thread\n",
    "    quit_serving.set()\n",
    "    if serving_thread.is_alive():\n",
    "        logger.bind(serving=True).info(\"Joining...\")\n",
    "        serving_thread.join( timeout = join_timeout )\n",
    "    if not serving_thread.is_alive():\n",
    "        print ('Joined serving thread',)\n",
    "        logger.bind(serving=True).info('Joined.')\n",
    "    else:\n",
    "        print ('Failed to join serving thread')\n",
    "\n",
    "def start_serving():\n",
    "    global quit_serving\n",
    "    global serving_thread\n",
    "    stop_serving()\n",
    "    quit_serving = mp.Event()\n",
    "    serving_thread = threading.Thread( target = serve, args = (quit_serving,), name = 'serving', daemon=True)\n",
    "    serving_thread.start()\n",
    "    logger.bind(serving=True).info(\"Started serving.\")\n",
    "    print('new serving thread:', serving_thread)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_serving()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (serving_thread.is_alive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_serving()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_serving_logs(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test-Serving** <a class=\"anchor\" id=\"Test-Serving\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axon_endpoint = bittensor.proto.Neuron(\n",
    "    address = bittensor.neuron.config.axon.external_ip,\n",
    "    port = bittensor.neuron.config.axon.external_port,\n",
    "    public_key = wallet.hotkey.public_key\n",
    ")\n",
    "start_time = time.time()\n",
    "codes, responses = bittensor.forward_text( \n",
    "    neurons = [ axon_endpoint ],\n",
    "    inputs = [ torch.tensor([[1]]) ]\n",
    ")\n",
    "end_time = time.time()\n",
    "print(colored('Querying endpoint: {}:{}'.format(axon_endpoint.address, axon_endpoint.port), 'blue'))\n",
    "if codes.item() == bittensor.proto.ReturnCode.Success:\n",
    "    print(colored('Success', 'green'))\n",
    "    print(colored('Response shape: {}'.format(responses[0].shape) , 'green'))\n",
    "    print(colored('Query time: {}'.format(end_time - start_time) , 'green'))\n",
    "else:\n",
    "    print(colored('Failure with code: {}'.format(codes.item()), 'red'))\n",
    "    print(colored('Ensure your axon is started with bittensor.axon.start()', 'red'))\n",
    "    print(colored('Ensure your endpoint is accessable from the internet, perhaps behind your router\\'s NAT?: {}', 'red'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Filtering Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the trained weights from the chain.\n",
    "weights_to_emit = model.row_weights.detach()\n",
    "\n",
    "# Take topk\n",
    "topk = 30\n",
    "weights_to_emit, indices = torch.topk(weights_to_emit, topk)\n",
    "\n",
    "# Normalize to 0,1\n",
    "weights_to_emit = F.normalize(weights_to_emit, p = 1, dim = 0)\n",
    "\n",
    "# Scatter back to size n.\n",
    "weights_to_emit = torch.scatter(torch.zeros(bittensor.metagraph.n()), 0, indices, weights_to_emit)\n",
    "print('Emitting weights: \\n{}'.format(weights_to_emit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets your incentive weights on the chain.\n",
    "bittensor.metagraph.set_weights( weights = weights_to_emit )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_uid = bittensor.metagraph.uids()[0]\n",
    "print ('Your weights (encoded as uint32s) on the chain are: \\n\\n {} \\n'.format(bittensor.subtensor.weight_vals_for_uid( your_uid )))\n",
    "\n",
    "print ('For uids \\n {}'.format(bittensor.subtensor.weight_uids_for_uid( your_uid )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transactions** <a class=\"anchor\" id=\"Transactions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Unstaking-Funds** <a class=\"anchor\" id=\"Unstaking-Funds\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bittensor.utils.balance import Balance\n",
    "amount_tao = 0.1\n",
    "amount = Balance.from_float( amount_tao )\n",
    "print(colored(\"Sending Extrinsic: [Unstake: {} Tao from hotkey: {}]\".format( amount.tao, wallet.hotkey.public_key) , 'blue'))\n",
    "print ('waiting for finalization...')\n",
    "result = bittensor.subtensor.unstake( amount, wallet.hotkey.public_key, wait_for_finalization = True, timeout = bittensor.__blocktime__ * 5)\n",
    "if result:\n",
    "    new_balance = bittensor.subtensor.get_balance(wallet.coldkeypub)\n",
    "    new_stake = bittensor.subtensor.get_stake_for_uid( bittensor.metagraph.uid_for_pubkey(wallet.hotkey.public_key) )\n",
    "    print(colored(\"Unstaked: {} Tao from hotkey: {} to coldkey.pub: {}\".format( amount.tao, wallet.hotkey.public_key, wallet.coldkey.public_key ) , 'green'))\n",
    "    print(colored(\"Your coldkey has new balance: {} Tao\".format( new_balance.tao ) , 'green'))\n",
    "    print(colored(\"Your hotkey has new stake: {} Tao\".format( new_stake.tao ) , 'green'))\n",
    "else:\n",
    "    print(colored(\"Unstaking transaction failed\", 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Staking-Funds** <a class=\"anchor\" id=\"Staking-Funds\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bittensor.utils.balance import Balance \n",
    "amount_tao = 0.1\n",
    "amount = Balance.from_float( amount_tao )\n",
    "print(colored(\"Sending Extrinsic: [Stake: {} Tao to hotkey: {}]\".format( amount.tao, wallet.hotkey.public_key) , 'blue'))\n",
    "print ('waiting for finalization...')\n",
    "result = bittensor.subtensor.add_stake( amount, wallet.hotkey.public_key, wait_for_finalization = True, timeout = bittensor.__blocktime__ * 5)\n",
    "if result:\n",
    "  new_balance = bittensor.subtensor.get_balance(wallet.coldkeypub)\n",
    "  new_stake = bittensor.subtensor.get_stake_for_uid(bittensor.metagraph.uid_for_pubkey(wallet.hotkey.public_key))\n",
    "  print(colored(\"Staked: {} Tao to hotkey: {} from coldkey.pub: {}\".format( amount.tao, wallet.hotkey.public_key, wallet.coldkey.public_key ) , 'green'))\n",
    "  print(colored(\"Your coldkey has new balance: {} Tao\".format( new_balance.tao ) , 'green'))\n",
    "  print(colored(\"Your hotkey has new stake: {} Tao\".format( new_stake.tao ) , 'green'))\n",
    "\n",
    "else:\n",
    "  print(colored(\"Staking transaction failed\", 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transfering-Funds** <a class=\"anchor\" id=\"Transfering-Funds\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = 0.01\n",
    "destination_public_key = wallet.coldkey.public_key\n",
    "amount = Balance.from_float( amount )\n",
    "balance = bittensor.subtensor.get_balance( wallet.coldkey.public_key )\n",
    "if balance < amount:\n",
    "    print(colored(\"Not enough balance ({}) to transfer {}\".format(balance, amount), 'red'))\n",
    "    quit()\n",
    "\n",
    "print(colored(\"Requesting transfer of {} Tao, from coldkey.pub: {} to dest.pub: {}\".format(amount.tao, wallet.coldkey.public_key, destination_public_key), 'blue'))\n",
    "print(\"Waiting for finalization...\",)\n",
    "result = bittensor.subtensor.transfer(destination_public_key, amount, wait_for_finalization = True, timeout = bittensor.__blocktime__ * 5)\n",
    "if result:\n",
    "    print(colored(\"Transfer finalized with amount: {} Tao to dest: {} from coldkey.pub: {}\".format(amount.tao, destination_public_key, wallet.coldkey.public_key), 'green'))\n",
    "    new_balance = bittensor.subtensor.get_balance(wallet.coldkeypub)\n",
    "    destination_balance = bittensor.subtensor.get_balance(destination_public_key)\n",
    "    print(colored(\"Your coldkey has new balance: {} Tao\".format( new_balance.tao ) , 'green'))\n",
    "    print(colored(\"The destination has new balance: {} Tao\".format( new_balance.tao ) , 'green'))\n",
    "else:\n",
    "    print(colored(\"Transfer failed\", 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
