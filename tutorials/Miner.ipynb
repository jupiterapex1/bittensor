{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bittensor\n",
    "import os\n",
    "import torch\n",
    "import torch.multiprocessing as mp \n",
    "import time\n",
    "from loguru import logger\n",
    "from termcolor import colored\n",
    "import nest_asyncio \n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set logging levels.\n",
    "bittensor.STDOUT_LOGGING_LEVEL = 'SUCCESS'\n",
    "bittensor.FILE_LOGGING_LEVEL = 'TRACE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Init Bittensor** <a class=\"anchor\" id=\"Init-Bittensor\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Wallet** <a class=\"anchor\" id=\"Wallet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WALLET: Holds keys to run your miner and control funds.\n",
    "\n",
    "# *****\n",
    "# IMPORTANT: Store the mnemonic for **both** your hot and coldkey \n",
    "# you will need these to recover your keys if you forget your password or lose access to this machine.\n",
    "# ******\n",
    "\n",
    "# Fill in below to name your wallet and keys.\n",
    "YOUR_WALLET_NAME = 'colab'\n",
    "YOUR_HOTKEY_NAME = 'colab_hot'\n",
    "\n",
    "# Fill in below if your need to regenerate your keys.\n",
    "use_mnemonic = False # Set to true for key regeneration.\n",
    "coldkey_mnemonic = \"<to be filled>\".split(' ')\n",
    "hotkey_mnemonic = \"<to be filled>\".split(' ')\n",
    "\n",
    "# Create the wallet object.\n",
    "wallet = bittensor.Wallet(\n",
    "    path = \"~/.bittensor/wallets/\",\n",
    "    name = YOUR_WALLET_NAME,\n",
    "    hotkey = YOUR_HOTKEY_NAME\n",
    ")\n",
    "\n",
    "# Optionally regens/creates your wallet keys.\n",
    "if not wallet.has_coldkeypub:\n",
    "    if use_mnemonic:\n",
    "        wallet.regenerate_coldkey(mnemonic = coldkey_mnemonic, use_password = True)\n",
    "    else:\n",
    "        wallet.create_new_coldkey(n_words = 12, use_password = True )\n",
    "if not wallet.has_hotkey:\n",
    "    if use_mnemonic:\n",
    "        wallet.regenerate_hotkey(mnemonic = hotkey_mnemonic)\n",
    "    else:\n",
    "        wallet.create_new_hotkey(n_words = 12)\n",
    "\n",
    "# Assert before continuing\n",
    "assert wallet.has_hotkey\n",
    "assert wallet.has_coldkeypub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Subtensor**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'subtensor' in locals():\n",
    "    del subtensor\n",
    "subtensor = bittensor.Subtensor(\n",
    "    subtensor_network = 'kusanagi'\n",
    ")\n",
    "print (bittensor.Config.toString(subtensor.config))\n",
    "subtensor.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Metagraph** <a class=\"anchor\" id=\"Metagraph\"></a> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'metagraph' in locals():\n",
    "    del metagraph\n",
    "metagraph = bittensor.Metagraph(\n",
    "    subtensor = subtensor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metagraph.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_block = metagraph.block()\n",
    "uids_on_chain = metagraph.uids()\n",
    "n_neurons = torch.max( metagraph.uids() )\n",
    "n_online = torch.numel(torch.where( metagraph.block() - metagraph.lastemit() < 1000 )[0])\n",
    "print ('The chain is at block: {}\\n'.format(metagraph.block()))\n",
    "print ('There are {} subscribed miner neurons \\n'.format(n_neurons))\n",
    "print ('These are their uids: \\n{}\\n'.format(metagraph.uids()))\n",
    "print ('{} have set weights in the last 1000 blocks\\n'.format(n_online))\n",
    "print ('\\u03C4{} is staked'.format(torch.sum(metagraph.S())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dendrite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'dendrite' in locals():\n",
    "    del dendrite\n",
    "dendrite = bittensor.Dendrite(\n",
    "    wallet = wallet,\n",
    "    dendrite_multiprocess = False,\n",
    "    receptor_do_backoff = False,\n",
    "    receptor_forward_timeout = 50,\n",
    "    receptor_backward_timeout = 50,\n",
    ")\n",
    "print (bittensor.Config.toString(dendrite.config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Nucleus**  <a class=\"anchor\" id=\"Nucleus\"></a>\n",
    "**a.k.a your unique machine learning model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GPT2-Nucleus** <a class=\"anchor\" id=\"GPT2-Nucleus\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from types import SimpleNamespace\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "\n",
    "class GPT2Nucleus(torch.nn.Module):\n",
    "    \"\"\" A simple-as-it-gets bittensor nucleus using a GPT2 kernel\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        huggingface_config = GPT2Config( vocab_size=bittensor.__vocab_size__, n_embd=bittensor.__network_dim__, n_layer=2, n_head=1, n_inner=8 )\n",
    "        self.transformer = GPT2Model(huggingface_config)\n",
    "        self.hidden_layer = nn.Linear( bittensor.__network_dim__, bittensor.__network_dim__ )\n",
    "        self.target_layer = nn.Linear( bittensor.__network_dim__, bittensor.__vocab_size__, bias=False )\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # The scores you learn for other neurons in the network.\n",
    "        self.row_weights = torch.ones([1], requires_grad=True)\n",
    "        \n",
    "        # Bittensor components that need to be set before you call remote_forward.\n",
    "        self.metagraph = None\n",
    "        self.dendrite = None\n",
    "    \n",
    "    \n",
    "    def local_forward(self, inputs: torch.LongTensor) -> SimpleNamespace:\n",
    "        \"\"\" Applies a forward pass to the model **without** queries to the network.\n",
    "        \"\"\"\n",
    "        # To be filled.\n",
    "        output = SimpleNamespace()\n",
    "\n",
    "        # Apply our GPT transformer model.\n",
    "        # local_context.shape = [ batch_size, sequence_len, network_dim ]\n",
    "        output.local_context = self.transformer(input_ids=inputs, return_dict=True).last_hidden_state\n",
    "\n",
    "        # Apply our dense layer and project it onto our hidden layer.\n",
    "        # local_hidden.shape = [ batch_size, sequence_len, network_dim ]\n",
    "        output.local_hidden = self.hidden_layer( output.local_context )\n",
    "\n",
    "        # Project to our target dimension.\n",
    "        # local_targets.shape = [ batch_size, sequence_len, vocab_size ]\n",
    "        output.local_targets = self.target_layer( output.local_hidden )\n",
    "\n",
    "        # Compute LM-loss \n",
    "        shift_targets = output.local_targets[..., :-1, :].contiguous()\n",
    "        shift_inputs = inputs[..., 1:].contiguous()\n",
    "        output.local_loss = self.loss_fct(shift_targets.view(-1, shift_targets.size(-1)), shift_inputs.view(-1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def remote_forward( self, inputs: torch.LongTensor, n_to_query:int = 10) -> SimpleNamespace:\n",
    "        \"\"\" Applies a forward pass to the model **with** queries to the network.\n",
    "        \"\"\"\n",
    "        # Sanity checks.\n",
    "        assert self.metagraph != None, 'you must assign model.metagraph, before you run a remote call.'\n",
    "        assert self.dendrite != None, 'you must assign model.dendrite, before you run a remote call.'\n",
    "        \n",
    "        # Run the local part of the model.\n",
    "        output = self.local_forward( inputs )\n",
    "\n",
    "        # Make queries to the network.\n",
    "        output = self.filter_and_make_queries( output, inputs, n_to_query )\n",
    "        \n",
    "        # Compute the distillation loss between the local and remote context (produced by the network query.)\n",
    "        output.distillation_loss = F.mse_loss( output.local_context, output.remote_context.detach() )\n",
    "\n",
    "        # Apply the hidden dense layer to the context.\n",
    "        # remote_hidden.shape = [ batch_size, sequence_len, network_dim ]\n",
    "        output.remote_hidden = self.hidden_layer( output.remote_context )\n",
    "\n",
    "        # Project onto our target dimension.\n",
    "        # remote_hidden.shape = [ batch_size, sequence_len, vocab_size ]\n",
    "        output.remote_targets = self.target_layer( output.remote_hidden )\n",
    "\n",
    "        # Compute our loss against the remote context.\n",
    "        shift_targets = output.remote_targets[..., :-1, :].contiguous()\n",
    "        shift_inputs = inputs[..., 1:].contiguous()\n",
    "        output.remote_loss = self.loss_fct(shift_targets.view(-1, shift_targets.size(-1)), shift_inputs.view(-1))\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def filter_and_make_queries(self, output: SimpleNamespace, inputs:torch.LongTensor, n_to_query:int = 10) -> SimpleNamespace:\n",
    "        \"\"\" Filters peers based on activity and makes RPC queries.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pad the row weights to the network dimension.\n",
    "        self.row_weights = torch.nn.functional.pad(\n",
    "            self.row_weights, \n",
    "            pad = [0, self.metagraph.n() - self.row_weights.numel() ],\n",
    "            value = torch.mean(self.row_weights).item() # New values at the mean.\n",
    "        ).clone().detach().requires_grad_(True)\n",
    "       \n",
    "        # Get all neuron uids.\n",
    "        # all_uids = [n]\n",
    "        all_uids = metagraph.uids() \n",
    "        \n",
    "        # Filter uids based on last emit.\n",
    "        # filtered_uids = [ m <= n]\n",
    "        filtered_uids = all_uids[ torch.where( metagraph.block() - metagraph.lastemit() < 1000 ) ] \n",
    "\n",
    "        # Get corresponding weights.\n",
    "        # filtered_weights = [ m ]\n",
    "        filtered_weights = self.row_weights[ filtered_uids ]\n",
    "\n",
    "        # Get topk weights for filtered uids\n",
    "        # query_indices = [ n_to_query <= m ]\n",
    "        gamma = 0.9\n",
    "        output.query_weights, indices = torch.topk(\n",
    "            filtered_weights + torch.rand_like(filtered_weights) * gamma, \n",
    "            min(n_to_query, torch.numel(filtered_weights))\n",
    "        )\n",
    "        \n",
    "        # Duplicate inputs for each request.\n",
    "        # inputs_to_send = n_to_query * [ batch_size, sequence_length ]\n",
    "        output.query_uids = filtered_uids[ indices.tolist() ]\n",
    "        inputs_to_send = [ inputs for _ in output.query_uids.tolist() ]\n",
    "        neurons_to_query = [ metagraph.neurons()[ i ] for i in output.query_uids.tolist() ]\n",
    "        \n",
    "        # Make network calls. \n",
    "        # responses = n_to_query * [batch_size, sequence_length, network_dimension]\n",
    "        output.codes, responses = self.dendrite.forward_text( \n",
    "            neurons = neurons_to_query, \n",
    "            inputs = inputs_to_send\n",
    "        )\n",
    "        \n",
    "        # Weight-join responses.\n",
    "        # remote_context = [batch_size, sequence_length, network dimension]\n",
    "        stacked_responses = torch.stack( responses, dim=2 )\n",
    "        output.remote_context = torch.matmul( torch.transpose( stacked_responses, dim0=2, dim1=3), output.query_weights)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your nucleus\n",
    "model = GPT2Nucleus()\n",
    "model.metagraph = metagraph\n",
    "model.dendrite = dendrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test-Nucleus** <a class=\"anchor\" id=\"Test-Nucleus\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test remote forward call.\n",
    "inputs = torch.tensor([ \n",
    "    bittensor.__tokenizer__()('the cat', max_length=10, truncation=True)['input_ids'],  # Text sequence 1\n",
    "])\n",
    "output = model.remote_forward( inputs, n_to_query = 50 )\n",
    "loss = output.local_loss + output.remote_loss + output.distillation_loss\n",
    "loss.backward() #\n",
    "print ('The joined remote context from the network is: \\n{}\\nwith shape: {}\\n'.format(output.remote_context, output.remote_context.shape))\n",
    "print ('The distillation loss between your local and remote context is  {}\\n'.format(output.distillation_loss))\n",
    "print ('The loss with respect to your local context and the targets is {}\\n'.format(output.local_loss))\n",
    "print ('The loss with respect to your remote context and the targets is {}\\n'.format(output.remote_loss))\n",
    "print ('You queried {} remote neurons with uids\\n {}\\n\\nand response codes\\n {}\\n'.format(torch.numel(output.codes), output.query_uids, output.codes.tolist()))\n",
    "print ('Gradients w.r.t your row weights: \\n', [float('{:0.3f}'.format(model.row_weights.grad[idx].item())) for idx in output.query_uids.tolist()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training** <a class=\"anchor\" id=\"Training\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training-Loop** <a class=\"anchor\" id=\"Training-Loop\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import os\n",
    "\n",
    "# ---- Dataset ---- \n",
    "dataset = load_dataset('ag_news')['train']\n",
    "def nextbatch(data, batch_size, tokenizer):\n",
    "    \"\"\" Returns a random batch of sentences from text dataset.\n",
    "    \"\"\"\n",
    "    batch_text = []\n",
    "    for _ in range(batch_size):\n",
    "        batch_text.append(data[random.randint(0, len(data))]['text'])\n",
    "    batch_inputs = tokenizer(batch_text, return_tensors='pt', max_length=10, padding=True, truncation=True)['input_ids']\n",
    "    return batch_inputs\n",
    "\n",
    "# --- Training Logger ----\n",
    "if 'training_logger' not in locals():\n",
    "    training_log_dir = os.path.expanduser('~/logs/training.log')\n",
    "    logger.add(training_log_dir, filter=lambda record: record[\"extra\"].get(\"name\") == \"training\", enqueue=True, backtrace=True, diagnose=True, rotation=\"500 MB\")\n",
    "    training_logger = logger.bind(name=\"training\")\n",
    "def show_training_logs(length: int = 25):\n",
    "    ! tail -n $length $training_log_dir\n",
    "\n",
    "# ---- Tokenizer ----\n",
    "# For encoding text inputs.\n",
    "tokenizer = bittensor.__tokenizer__()\n",
    "\n",
    "# ---- Optimizer ----\n",
    "# For applying gradient steps to the local model.\n",
    "optimizer = torch.optim.SGD( model.parameters(), lr = 0.1, momentum = 0.99 )\n",
    "\n",
    "# ---- Training Loop -----\n",
    "def train( \n",
    "        stop_training: mp.Event,\n",
    "    ):\n",
    "    # ---- Loop until event is set ----\n",
    "    training_step = 0\n",
    "    batch_size = 1\n",
    "    logger.bind(training=True).info('Loop starting... ')\n",
    "    while not stop_training.is_set():\n",
    "        try:\n",
    "            optimizer.zero_grad() # Zeros out gradients for next accummulation\n",
    "\n",
    "            # ---- Forward pass ----\n",
    "            inputs = nextbatch( dataset, batch_size, tokenizer )\n",
    "            output = model.remote_forward( inputs, n_to_query=10 )\n",
    "\n",
    "            # ---- Backward pass ----\n",
    "            loss = output.local_loss + output.remote_loss + output.distillation_loss\n",
    "            loss.backward() # Accumulates gradients on the model.\n",
    "            clip_grad_norm_(model.parameters(), 0.8) # clip model gradients\n",
    "            optimizer.step() # Applies accumulated gradients.\n",
    "\n",
    "            # ---- Step logs ----\n",
    "            training_logger.info('->\\nuids:{}\\ncodes:{}\\nweights:{}\\ngrads:{}', \n",
    "                  output.query_uids, \n",
    "                  output.codes.tolist(), \n",
    "                  [float('{:0.3f}'.format(x)) for x in output.query_weights.tolist()],\n",
    "                  [float('{:0.3f}'.format(model.row_weights.grad[idx].item())) for idx in output.query_uids.tolist()])      \n",
    "            training_logger.info('gs:{} loss(local):{} loss(remote):{} loss(distill):{} dendrite:{}',\n",
    "                  colored('{}'.format( training_step ), 'red'),\n",
    "                  colored('{:.4f}'.format(output.local_loss.item()), 'green'),\n",
    "                  colored('{:.4f}'.format(output.remote_loss.item()), 'blue'),\n",
    "                  colored('{:.4f}'.format(output.distillation_loss.item()), 'red'),\n",
    "                  dendrite)\n",
    "\n",
    "            # --- Train and normalize row weights ---\n",
    "            model.row_weights = torch.clamp(model.row_weights - 0.001 * model.row_weights.grad, 0, 1)\n",
    "            model.row_weights = F.normalize( model.row_weights, p = 1, dim = 0 ).clone().detach().requires_grad_(True)\n",
    "\n",
    "            training_step += 1\n",
    "        except Exception as e:\n",
    "            training_logger.exception(\"Training iteration exception.\")\n",
    "    logger.bind(training=True).complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training Thread Runners** <a class=\"anchor\" id=\"Training-Thread-Runners\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import torch.multiprocessing as mp \n",
    "import sys\n",
    "\n",
    "join_timeout = 10\n",
    "\n",
    "if 'quit_training' in locals():\n",
    "    quit_training.set()\n",
    "if 'training_thread' in locals() and training_thread.is_alive():\n",
    "    training_thread.join( timeout = join_timeout )\n",
    "\n",
    "quit_training = mp.Event()\n",
    "training_thread = threading.Thread( target = train, args = (quit_training,),  name = 'training', daemon=True)\n",
    "\n",
    "def stop_training():\n",
    "    global quit_training\n",
    "    global training_thread\n",
    "    quit_training.set()\n",
    "    if not training_thread.is_alive():\n",
    "        return\n",
    "    training_logger.info(\"Joining...\")\n",
    "    training_thread.join( timeout = join_timeout )\n",
    "    if not training_thread.is_alive():\n",
    "        print ('Joined training thread',)\n",
    "        training_logger.info('Joined.')\n",
    "    else:\n",
    "        print ('Failed to join training thread')\n",
    "\n",
    "def start_training():\n",
    "    global quit_training\n",
    "    global training_thread\n",
    "    stop_training()\n",
    "    quit_training = mp.Event()\n",
    "    training_thread = threading.Thread( target = train, args = (quit_training,), name = 'training', daemon=True)\n",
    "    training_thread.start()\n",
    "    training_logger.info(\"Started training.\")\n",
    "    print('new training thread:', training_thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (training_thread.is_alive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_training_logs(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Serving** <a class=\"anchor\" id=\"Serving\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'axon' in locals():\n",
    "    del axon\n",
    "axon = bittensor.Axon(\n",
    "    axon_local_ip = '127.0.0.1',\n",
    "    axon_local_port = 8082,\n",
    "    axon_external_port = 8082,\n",
    "    axon_external_ip = bittensor.utils.networking.get_external_ip(),\n",
    ")\n",
    "print (bittensor.Config.toString(axon.config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the axon serving endpoint.\n",
    "axon.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axon.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tail -n 10 '~/.bittensor/logs.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc \n",
    "channel = grpc.insecure_channel(\n",
    "            '127.0.0.1:8082',\n",
    "            options=[('grpc.max_send_message_length', -1),\n",
    "                     ('grpc.max_receive_message_length', -1)])\n",
    "stub = bittensor.grpc.BittensorStub( channel )\n",
    "\n",
    "\n",
    "inputs_raw = torch.tensor( [ [ 1 ] ], dtype=torch.int64)\n",
    "serializer = bittensor.serialization.get_serializer( serialzer_type = bittensor.proto.Serializer.MSGPACK )\n",
    "inputs_serialized = serializer.serialize(inputs_raw, modality = bittensor.proto.Modality.TEXT, from_type = bittensor.proto.TensorType.TORCH)\n",
    "request = bittensor.proto.TensorMessage(\n",
    "    version = bittensor.__version__,\n",
    "    public_key = 'sssss',\n",
    "    tensors = [inputs_serialized]\n",
    ")\n",
    "\n",
    "response = stub.Forward(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Serving-Loop** <a class=\"anchor\" id=\"Serving-Loop\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Serving logger ----\n",
    "if 'serving_logger' not in locals():\n",
    "    serving_log_dir = os.path.expanduser('~/logs/serving.log')\n",
    "    logger.add(serving_log_dir, colorize=True, filter=lambda record: record[\"extra\"].get(\"name\") == \"serving\", enqueue=True, backtrace=True, diagnose=True, rotation=\"500 MB\")\n",
    "    serving_logger = logger.bind(name=\"serving\")\n",
    "\n",
    "def show_serving_logs(length: int = 25):\n",
    "    ! tail -n $length $serving_log_dir\n",
    "\n",
    "# ---- Serving loop -----\n",
    "def serve ( \n",
    "  stop_serving: mp.Event,\n",
    "):\n",
    "\n",
    "    # ---- Loop until event is set -----\n",
    "    serving_step = 0\n",
    "    serving_logger.info('Serving thread started: ')\n",
    "    while not stop_serving.is_set():\n",
    "        try:\n",
    "            \n",
    "            # ---- Pull request ----\n",
    "            serving_logger.info('Axon:{}, waiting for query ... ', axon)\n",
    "            pong, pubkey, inputs, modality = axon.next_forward_item( timeout = 10.0 )\n",
    "\n",
    "            # ---- Process request ----\n",
    "            if None not in [ pong, pubkey, inputs, modality ]:\n",
    "                serving_logger.info('Recieved Query: from:{}, inputs.shape:{}', pubkey, inputs.shape)\n",
    "                output = model.local_forward( inputs ).local_hidden\n",
    "                pong.send( output.detach() )\n",
    "                serving_logger.info('Sent response: to:{}, output.shape:{}', pubkey, output.shape)\n",
    "\n",
    "        except Exception as e:\n",
    "            serving_logger.exception('Error in forward process with error {}', e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Serving Thread Runners** <a class=\"anchor\" id=\"Serving-Thread-Runners\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "join_timeout = 10\n",
    "\n",
    "if 'quit_serving' in locals():\n",
    "    quit_serving.set()\n",
    "if 'serving_thread' in locals() and serving_thread.is_alive():\n",
    "    serving_thread.join( timeout = join_timeout )\n",
    "\n",
    "quit_serving = mp.Event()\n",
    "serving_thread = threading.Thread( target = serve, args = (quit_serving,),  name = 'serving', daemon=True )\n",
    "\n",
    "def stop_serving():\n",
    "    global quit_serving\n",
    "    global serving_thread\n",
    "    quit_serving.set()\n",
    "    if serving_thread.is_alive():\n",
    "        serving_logger.info(\"Joining...\")\n",
    "        serving_thread.join( timeout = join_timeout )\n",
    "    if not serving_thread.is_alive():\n",
    "        print ('Joined serving thread',)\n",
    "        serving_logger.info('Joined.')\n",
    "    else:\n",
    "        print ('Failed to join serving thread')\n",
    "\n",
    "def start_serving():\n",
    "    global quit_serving\n",
    "    global serving_thread\n",
    "    stop_serving()\n",
    "    quit_serving = mp.Event()\n",
    "    serving_thread = threading.Thread( target = serve, args = (quit_serving,), name = 'serving', daemon=True )\n",
    "    serving_thread.start()\n",
    "    serving_logger.info(\"Started serving.\")\n",
    "    print('new serving thread:', serving_thread)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_serving()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (serving_thread.is_alive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_serving()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_serving_logs(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = bittensor.proto.Neuron(\n",
    "    address = axon.config.axon.local_ip,\n",
    "    port = axon.config.axon.local_port,\n",
    "    public_key = wallet.hotkey.public_key\n",
    ")\n",
    "start_time = time.time()\n",
    "codes, responses = dendrite.forward_text( \n",
    "    neurons = [ endpoint ],\n",
    "    inputs = [ torch.tensor([[1]]) ]\n",
    ")\n",
    "end_time = time.time()\n",
    "print(colored('Querying endpoint: {}:{}'.format(axon.config.axon.local_ip, axon.config.axon.local_port), 'blue'))\n",
    "if codes.item() == bittensor.proto.ReturnCode.Success:\n",
    "    print(colored('Success', 'green'))\n",
    "    print(colored('Response shape: {}'.format(responses[0].shape) , 'green'))\n",
    "    print(colored('Query time: {}'.format(end_time - start_time) , 'green'))\n",
    "else:\n",
    "    print(colored('Failure with code: {}'.format(codes.item()), 'red'))\n",
    "    print(colored('Ensure your axon is started with axon.start()', 'red'))\n",
    "    print(colored('Ensure your endpoint is accessible from the internet, perhaps behind your router\\'s NAT?', 'red'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Filtering Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the trained weights from the chain.\n",
    "weights_to_emit = torch.nn.functional.pad(\n",
    "    model.row_weights, \n",
    "    pad = [0, metagraph.n() - model.row_weights.numel() ],\n",
    "    value = torch.mean(model.row_weights).item()\n",
    ")\n",
    "\n",
    "model.row_weights.detach()\n",
    "\n",
    "# Take topk\n",
    "topk = 30\n",
    "weights_to_emit, uids = torch.topk(weights_to_emit, topk)\n",
    "\n",
    "# Normalize to 0,1\n",
    "weights_to_emit = F.normalize(weights_to_emit, p = 1, dim = 0)\n",
    "print (\"Weights:\\n{}\\nFor uids\\n {}\".format(weights_to_emit.tolist(), uids.tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets your incentive weights on the chain.\n",
    "subtensor.set_weights(\n",
    "    uids = uids,\n",
    "    weights = weights_to_emit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_uid = bittensor.metagraph.uids()[0]\n",
    "print ('Your weights (encoded as uint32s) on the chain are: \\n\\n {} \\n'.format(bittensor.subtensor.weight_vals_for_uid( your_uid )))\n",
    "\n",
    "print ('For uids \\n {}'.format(bittensor.subtensor.weight_uids_for_uid( your_uid )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transactions** <a class=\"anchor\" id=\"Transactions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Unstaking-Funds** <a class=\"anchor\" id=\"Unstaking-Funds\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bittensor.utils.balance import Balance\n",
    "amount_tao = 0.1\n",
    "amount = Balance.from_float( amount_tao )\n",
    "print(colored(\"Sending Extrinsic: [Unstake: \\u03C4{} from hotkey: {}]\".format( amount.tao, wallet.hotkey.public_key) , 'blue'))\n",
    "print ('waiting for finalization...')\n",
    "result = subtensor.unstake( amount, wallet.hotkey.public_key, wait_for_finalization = True, timeout = bittensor.__blocktime__ * 5)\n",
    "if result:\n",
    "    new_balance = subtensor.get_balance(wallet.coldkeypub)\n",
    "    new_stake = subtensor.get_stake_for_uid( metagraph.uid_for_pubkey(wallet.hotkey.public_key) )\n",
    "    print(colored(\"Unstaked: \\u03C4{}\\nfrom hotkey: {}\\nto coldkey.pub: {}\".format( amount.tao, wallet.hotkey.public_key, wallet.coldkey.public_key ) , 'green'))\n",
    "    print(colored(\"Your coldkey has new balance: \\u03C4{}\".format( new_balance.tao ) , 'green'))\n",
    "    print(colored(\"Your hotkey has new stake: \\u03C4{}\".format( new_stake.tao ) , 'green'))\n",
    "else:\n",
    "    print(colored(\"Unstaking transaction failed\", 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Staking-Funds** <a class=\"anchor\" id=\"Staking-Funds\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mSending Extrinsic: [Stake: 0.1 Tao to hotkey: 0x80cacfbdf7b155b39de22680a7cb14c61a8f95df702c92f5f142d25cca37c545]\u001b[0m\n",
      "waiting for finalization...\n",
      "\u001b[32mStaked: τ0.1\n",
      "to hotkey: 0x80cacfbdf7b155b39de22680a7cb14c61a8f95df702c92f5f142d25cca37c545\n",
      "from coldkey.pub: 0x3c9cd1679888e5660b0c8e4b8a17a1719c0cb7f05b5c624a856b421b52290515\u001b[0m\n",
      "\u001b[32mYour coldkey has new balance: τ0.1\u001b[0m\n",
      "\u001b[32mYour hotkey has new stake: τ254.839397709\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from bittensor.utils.balance import Balance \n",
    "amount_tao = 0.1\n",
    "amount = Balance.from_float( amount_tao )\n",
    "print(colored(\"Sending Extrinsic: [Stake: {} Tao to hotkey: {}]\".format( amount.tao, wallet.hotkey.public_key ) , 'blue'))\n",
    "print ('waiting for finalization...')\n",
    "result = subtensor.add_stake( amount, wallet.hotkey.public_key, wait_for_finalization = True, timeout = bittensor.__blocktime__ * 5)\n",
    "if result:\n",
    "    new_balance = subtensor.get_balance(wallet.coldkeypub)\n",
    "    new_stake = subtensor.get_stake_for_uid( metagraph.uid_for_pubkey(wallet.hotkey.public_key))\n",
    "    print(colored(\"Staked: \\u03C4{}\\nto hotkey: {}\\nfrom coldkey.pub: {}\".format( amount.tao, wallet.hotkey.public_key, wallet.coldkey.public_key ) , 'green'))\n",
    "    print(colored(\"Your coldkey has new balance: \\u03C4{}\".format( new_balance.tao ) , 'green'))\n",
    "    print(colored(\"Your hotkey has new stake: \\u03C4{}\".format( new_stake.tao ) , 'green'))\n",
    "else:\n",
    "    print(colored(\"Staking transaction failed\", 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2021-04-08 10:43:18.535\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mbittensor.wallet\u001b[0m:\u001b[36m_load_hotkey\u001b[0m:\u001b[36m276\u001b[0m - \u001b[32m\u001b[1mLoaded hotkey: 0x80cacfbdf7b155b39de22680a7cb14c61a8f95df702c92f5f142d25cca37c545</green>\n",
      "\u001b[0m\n",
      "\u001b[32m2021-04-08 10:43:48.259\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mbittensor.wallet\u001b[0m:\u001b[36m_load_hotkey\u001b[0m:\u001b[36m276\u001b[0m - \u001b[32m\u001b[1mLoaded hotkey: 0x80cacfbdf7b155b39de22680a7cb14c61a8f95df702c92f5f142d25cca37c545</green>\n",
      "\u001b[0m\n",
      "\u001b[32m2021-04-08 10:44:34.384\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mbittensor.wallet\u001b[0m:\u001b[36m_load_hotkey\u001b[0m:\u001b[36m276\u001b[0m - \u001b[32m\u001b[1mLoaded hotkey: 0x80cacfbdf7b155b39de22680a7cb14c61a8f95df702c92f5f142d25cca37c545</green>\n",
      "\u001b[0m\n",
      "\u001b[32m2021-04-08 10:56:01.599\u001b[0m | \u001b[41m\u001b[1mCRITICAL\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[41m\u001b[1mhello\u001b[0m\n",
      "\u001b[32m2021-04-08 10:56:01.600\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[32m\u001b[1mhello\u001b[0m\n",
      "\u001b[32m2021-04-08 10:56:01.600\u001b[0m | \u001b[34mfoobar  \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[34mhello\u001b[0m\n",
      "\u001b[32m2021-04-08 12:15:02.433\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mbittensor.subtensor\u001b[0m:\u001b[36m_submit_and_check_extrinsic\u001b[0m:\u001b[36m336\u001b[0m - \u001b[31m\u001b[1mError in extrinsic: No response within timeout\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! tail -n 10 '~/.bittensor/logs.log'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Transfering-Funds** <a class=\"anchor\" id=\"Transfering-Funds\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = 0.01\n",
    "destination_public_key = wallet.coldkey.public_key\n",
    "amount = Balance.from_float( amount )\n",
    "balance = bittensor.subtensor.get_balance( wallet.coldkey.public_key )\n",
    "if balance < amount:\n",
    "    print(colored(\"Not enough balance \\u03C4{} to transfer \\u03C4{}\".format(balance, amount), 'red'))\n",
    "    quit()\n",
    "\n",
    "print(colored(\"Requesting transfer of \\u03C4{}, from coldkey.pub: {} to dest.pub: {}\".format(amount.tao, wallet.coldkey.public_key, destination_public_key), 'blue'))\n",
    "print(\"Waiting for finalization...\",)\n",
    "result = bittensor.subtensor.transfer(destination_public_key, amount, wait_for_finalization = True, timeout = bittensor.__blocktime__ * 5)\n",
    "if result:\n",
    "    print(colored(\"Transfer finalized with amount: \\u03C4{} to dest: {} from coldkey.pub: {}\".format(amount.tao, destination_public_key, wallet.coldkey.public_key), 'green'))\n",
    "    new_balance = bittensor.subtensor.get_balance(wallet.coldkeypub)\n",
    "    destination_balance = bittensor.subtensor.get_balance(destination_public_key)\n",
    "    print(colored(\"Your coldkey has new balance: \\u03C4{}\".format( new_balance.tao ) , 'green'))\n",
    "    print(colored(\"The destination has new balance: \\u03C4{}\".format( new_balance.tao ) , 'green'))\n",
    "else:\n",
    "    print(colored(\"Transfer failed\", 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
