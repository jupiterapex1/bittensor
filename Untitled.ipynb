{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/const/Workspace/bittensor\n",
      "Collecting argparse\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: autobahn==20.7.1 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (20.7.1)\n",
      "Requirement already satisfied: base58>=2.0.1 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2020.11.8 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (2020.12.5)\n",
      "Requirement already satisfied: cryptography==3.1.1 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (3.1.1)\n",
      "Requirement already satisfied: datasets in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (1.1.0)\n",
      "Requirement already satisfied: cython in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (0.29.21)\n",
      "Requirement already satisfied: docker==4.3.1 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (4.3.1)\n",
      "Requirement already satisfied: idna>=2.10 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (2.10)\n",
      "Requirement already satisfied: google-api-python-client in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (1.12.3)\n",
      "Requirement already satisfied: grpcio==1.32.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (1.32.0)\n",
      "Requirement already satisfied: grpcio-tools==1.32.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (1.32.0)\n",
      "Requirement already satisfied: loguru in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (0.5.3)\n",
      "Requirement already satisfied: msgpack==1.0.2 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (1.0.2)\n",
      "Requirement already satisfied: msgpack-numpy==0.4.7.1 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (0.4.7.1)\n",
      "Requirement already satisfied: miniupnpc in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (2.0.2)\n",
      "Requirement already satisfied: munch in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (2.5.0)\n",
      "Requirement already satisfied: netaddr==0.8.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (0.8.0)\n",
      "Requirement already satisfied: password_strength==0.0.3.post2 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (0.0.3.post2)\n",
      "Requirement already satisfied: pickle-mixin==1.0.2 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (1.0.2)\n",
      "Requirement already satisfied: prettytable in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (2.0.0)\n",
      "Requirement already satisfied: protobuf==3.13.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (3.13.0)\n",
      "Requirement already satisfied: pycryptodome==3.9.8 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (3.9.8)\n",
      "Requirement already satisfied: pycrypto==2.6.1 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (2.6.1)\n",
      "Requirement already satisfied: py-sr25519-bindings==0.1.2 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (0.1.2)\n",
      "Requirement already satisfied: py-ed25519-bindings~=0.1.2 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (0.1.2)\n",
      "Requirement already satisfied: py-bip39-bindings>=0.1.6 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (0.1.6)\n",
      "Requirement already satisfied: pytest>=6.1.2 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (6.1.2)\n",
      "Requirement already satisfied: pytest-xdist==2.2.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (2.2.0)\n",
      "Requirement already satisfied: pytest-asyncio>=0.14.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (0.14.0)\n",
      "Requirement already satisfied: pytorch_transformers in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (1.2.0)\n",
      "Requirement already satisfied: pyyaml in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (5.3.1)\n",
      "Requirement already satisfied: rollbar in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (0.15.1)\n",
      "Requirement already satisfied: requests~=2.25.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (2.25.0)\n",
      "Requirement already satisfied: scalecodec~=0.10.59 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (0.10.60)\n",
      "Requirement already satisfied: termcolor in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (1.1.0)\n",
      "Requirement already satisfied: tensorboard in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (2.2.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (1.6.0)\n",
      "Requirement already satisfied: torchvision in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (0.7.0)\n",
      "Requirement already satisfied: transformers in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (4.2.2)\n",
      "Requirement already satisfied: validators in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (0.18.1)\n",
      "Requirement already satisfied: xxhash>=2.0.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (2.0.0)\n",
      "Requirement already satisfied: pandas in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (1.1.2)\n",
      "Requirement already satisfied: numpy in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (1.19.2)\n",
      "Requirement already satisfied: wheel in /Users/const/.local/lib/python3.7/site-packages (from bittensor==1.0.4) (0.36.2)\n",
      "Requirement already satisfied: codecov in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from bittensor==1.0.4) (2.1.10)\n",
      "Requirement already satisfied: txaio>=20.3.1 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from autobahn==20.7.1->bittensor==1.0.4) (20.4.1)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from cryptography==3.1.1->bittensor==1.0.4) (1.14.3)\n",
      "Requirement already satisfied: six>=1.4.1 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from cryptography==3.1.1->bittensor==1.0.4) (1.15.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from docker==4.3.1->bittensor==1.0.4) (0.57.0)\n",
      "Requirement already satisfied: setuptools in /Users/const/.local/lib/python3.7/site-packages (from protobuf==3.13.0->bittensor==1.0.4) (51.3.3)\n",
      "Requirement already satisfied: pytest-forked in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pytest-xdist==2.2.0->bittensor==1.0.4) (1.3.0)\n",
      "Requirement already satisfied: execnet>=1.1 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pytest-xdist==2.2.0->bittensor==1.0.4) (1.7.1)\n",
      "Requirement already satisfied: pycparser in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography==3.1.1->bittensor==1.0.4) (2.20)\n",
      "Requirement already satisfied: apipkg>=1.4 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from execnet>=1.1->pytest-xdist==2.2.0->bittensor==1.0.4) (1.5)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pytest>=6.1.2->bittensor==1.0.4) (0.13.1)\n",
      "Requirement already satisfied: toml in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pytest>=6.1.2->bittensor==1.0.4) (0.10.1)\n",
      "Requirement already satisfied: packaging in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pytest>=6.1.2->bittensor==1.0.4) (20.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pytest>=6.1.2->bittensor==1.0.4) (20.2.0)\n",
      "Requirement already satisfied: py>=1.8.2 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pytest>=6.1.2->bittensor==1.0.4) (1.9.0)\n",
      "Requirement already satisfied: iniconfig in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pytest>=6.1.2->bittensor==1.0.4) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pytest>=6.1.2->bittensor==1.0.4) (2.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zipp>=0.5 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest>=6.1.2->bittensor==1.0.4) (3.3.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from requests~=2.25.0->bittensor==1.0.4) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from requests~=2.25.0->bittensor==1.0.4) (1.26.2)\n",
      "Requirement already satisfied: more-itertools in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from scalecodec~=0.10.59->bittensor==1.0.4) (8.6.0)\n",
      "Requirement already satisfied: future in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from torch>=1.6.0->bittensor==1.0.4) (0.18.2)\n",
      "Requirement already satisfied: coverage in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from codecov->bittensor==1.0.4) (5.3)\n",
      "Requirement already satisfied: multiprocess in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from datasets->bittensor==1.0.4) (0.70.10)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from datasets->bittensor==1.0.4) (3.0.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from datasets->bittensor==1.0.4) (4.50.0)\n",
      "Requirement already satisfied: filelock in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from datasets->bittensor==1.0.4) (3.0.12)\n",
      "Requirement already satisfied: dill in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from datasets->bittensor==1.0.4) (0.3.2)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from google-api-python-client->bittensor==1.0.4) (3.0.1)\n",
      "Requirement already satisfied: google-auth>=1.16.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from google-api-python-client->bittensor==1.0.4) (1.22.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from google-api-python-client->bittensor==1.0.4) (0.18.1)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from google-api-python-client->bittensor==1.0.4) (1.22.3)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from google-api-python-client->bittensor==1.0.4) (0.0.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->bittensor==1.0.4) (1.52.0)\n",
      "Requirement already satisfied: pytz in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->bittensor==1.0.4) (2020.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0dev,>=3.6.2 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from google-auth>=1.16.0->google-api-python-client->bittensor==1.0.4) (3.6.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from google-auth>=1.16.0->google-api-python-client->bittensor==1.0.4) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from google-auth>=1.16.0->google-api-python-client->bittensor==1.0.4) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from google-auth>=1.16.0->google-api-python-client->bittensor==1.0.4) (0.2.8)\n",
      "Requirement already satisfied: multidict<5.0,>=4.5 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth>=1.16.0->google-api-python-client->bittensor==1.0.4) (4.7.6)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth>=1.16.0->google-api-python-client->bittensor==1.0.4) (1.6.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth>=1.16.0->google-api-python-client->bittensor==1.0.4) (3.0.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client->bittensor==1.0.4) (0.4.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0dev,>=3.6.2->google-auth>=1.16.0->google-api-python-client->bittensor==1.0.4) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from packaging->pytest>=6.1.2->bittensor==1.0.4) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pandas->bittensor==1.0.4) (2.8.1)\n",
      "Requirement already satisfied: wcwidth in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from prettytable->bittensor==1.0.4) (0.2.5)\n",
      "Requirement already satisfied: sacremoses in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pytorch_transformers->bittensor==1.0.4) (0.0.43)\n",
      "Requirement already satisfied: regex in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pytorch_transformers->bittensor==1.0.4) (2020.9.27)\n",
      "Requirement already satisfied: boto3 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pytorch_transformers->bittensor==1.0.4) (1.16.47)\n",
      "Requirement already satisfied: sentencepiece in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from pytorch_transformers->bittensor==1.0.4) (0.1.91)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from boto3->pytorch_transformers->bittensor==1.0.4) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from boto3->pytorch_transformers->bittensor==1.0.4) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.47 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from boto3->pytorch_transformers->bittensor==1.0.4) (1.19.47)\n",
      "Requirement already satisfied: joblib in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from sacremoses->pytorch_transformers->bittensor==1.0.4) (0.17.0)\n",
      "Requirement already satisfied: click in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from sacremoses->pytorch_transformers->bittensor==1.0.4) (7.1.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from tensorboard->bittensor==1.0.4) (3.2.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from tensorboard->bittensor==1.0.4) (0.10.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from tensorboard->bittensor==1.0.4) (0.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from tensorboard->bittensor==1.0.4) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from tensorboard->bittensor==1.0.4) (1.0.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->bittensor==1.0.4) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->bittensor==1.0.4) (3.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow>=4.1.1 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from torchvision->bittensor==1.0.4) (7.2.0)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from transformers->bittensor==1.0.4) (0.9.4)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /Users/const/.pyenv/versions/3.7.3/lib/python3.7/site-packages (from validators->bittensor==1.0.4) (4.4.2)\n",
      "Installing collected packages: argparse, bittensor\n",
      "  Attempting uninstall: bittensor\n",
      "    Found existing installation: bittensor 1.0.4\n",
      "    Uninstalling bittensor-1.0.4:\n",
      "      Successfully uninstalled bittensor-1.0.4\n",
      "  Running setup.py develop for bittensor\n",
      "Successfully installed argparse-1.4.0 bittensor\n"
     ]
    }
   ],
   "source": [
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bittensor\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import torch.multiprocessing as mp \n",
    "import queue\n",
    "import nest_asyncio \n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mLoaded coldkey.pub: 0x3c9cd1679888e5660b0c8e4b8a17a1719c0cb7f05b5c624a856b421b52290515\u001b[0m\n",
      "\u001b[32mLoaded hotkey: 0x80cacfbdf7b155b39de22680a7cb14c61a8f95df702c92f5f142d25cca37c545\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# WALLET: Keys for running your miner and controlling funds.\n",
    "\n",
    "# *****\n",
    "# IMPORTANT: Store the mnemonic for **both** your hot and coldkey \n",
    "# you will need these if you want to recover the files off this machine.\n",
    "# ******\n",
    "\n",
    "# Fill in below to name your wallet and keys.\n",
    "YOUR_WALLET_NAME = 'colab'\n",
    "YOUR_HOTKEY_NAME = 'colab_hot'\n",
    "\n",
    "# Fill in below if your need to regenerate your keys.\n",
    "use_mnemonic = True # Set to true for key regeneration.\n",
    "coldkey_mnemonic = \"fancy catalog grant scatter summer minute luxury gym spot taxi theme initial\"\n",
    "hotkey_mnemonic = \"away canvas cost drip soldier retreat match choice inject envelope grit asset\"\n",
    "\n",
    "# Create the wallet object.\n",
    "wallet = bittensor.wallet.Wallet(\n",
    "    path = \"~/.bittensor/wallets/\",\n",
    "    name = YOUR_WALLET_NAME,\n",
    "    hotkey = YOUR_HOTKEY_NAME\n",
    ")\n",
    "\n",
    "# Optionally regens/creates your wallet keys.\n",
    "if not wallet.has_coldkeypub:\n",
    "    if use_mnemonic:\n",
    "        wallet.regenerate_coldkey(mnemonic = coldkey_mnemonic, use_password = True)\n",
    "    else:\n",
    "        wallet.create_new_coldkey(n_words = 12, use_password = True )\n",
    "if not wallet.has_hotkey:\n",
    "    if use_mnemonic:\n",
    "        wallet.regenerate_hotkey(mnemonic = hotkey_mnemonic, use_password = True)\n",
    "    else:\n",
    "        wallet.create_new_hotkey(n_words = 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO    |bittensor.axon:check_config:198 - UPNPC: OFF\n",
      "INFO    |bittensor.axon:check_config:201 - Using external endpoint: 132.191.3.149:8091\n",
      "INFO    |bittensor.axon:check_config:202 - Using local endpoint: 127.0.0.1:8091\n",
      "INFO    |bittensor.neuron:stop:181 - Shutting down the Axon server ...\n",
      "INFO    |bittensor.neuron:stop:184 - Axon server stopped\n"
     ]
    }
   ],
   "source": [
    "# Subtensor: Connection to the blockchain\n",
    "# Needed for token transfers and querying network state.\n",
    "subtensor = bittensor.subtensor.Subtensor(\n",
    "    wallet = wallet, # Used for signing transactions.\n",
    "    network = 'kusanagi' # 'akira' for testnet, 'kusanagi' for mainnet\n",
    ")\n",
    "\n",
    "# Metagraph: Chain state object.\n",
    "metagraph = bittensor.metagraph.Metagraph(\n",
    "    wallet = wallet, # Identifies yourself.\n",
    "    subtensor = subtensor # Chain connection for polling state. \n",
    ")\n",
    "\n",
    "# Dendrite: Tensor RPC client.\n",
    "dendrite = bittensor.dendrite.Dendrite(\n",
    "    wallet = wallet, # Used for signing RPC requests.\n",
    "    metagraph = metagraph, # Used for identifying peers.\n",
    ")\n",
    "\n",
    "# Axon: Tensor RPC server.\n",
    "axon = bittensor.axon.Axon(\n",
    "    wallet = wallet, # Used for signing RPC responses.\n",
    "    metagraph = metagraph, # Used for identifying peers.\n",
    ")\n",
    "\n",
    "neuron = bittensor.neuron.Neuron(\n",
    "    wallet = wallet,\n",
    "    metagraph = metagraph,\n",
    "    dendrite = dendrite,\n",
    "    axon = axon,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Model.\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "from torch import nn\n",
    "from munch import Munch\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import bittensor\n",
    "from routers.pkm import PKMRouter\n",
    "\n",
    "class GPT2Pooler(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class GPT2LMNucleus(bittensor.nucleus.Nucleus):\n",
    "    \"\"\" A Bittensor Synapse training GPT2 with Causal Language Modelling (CLM)\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Munch = None, **kwargs):\n",
    "        r\"\"\" Init a new GPT2 synapse module.\n",
    "\n",
    "            Args:\n",
    "                config (:obj:`munch.Munch`, `required`): \n",
    "                    munched config class.\n",
    "        \"\"\"\n",
    "        super(GPT2LMNucleus, self).__init__(config = config, **kwargs)\n",
    "        if config == None:\n",
    "            config = GPT2LMNucleus.default_config()\n",
    "        bittensor.config.Config.update_with_kwargs(config.synapse, kwargs) \n",
    "        GPT2LMNucleus.check_config(config)\n",
    "        self.config = config\n",
    "\n",
    "        # Build huggingface config.\n",
    "        huggingface_config = GPT2Config(\n",
    "                vocab_size=bittensor.__vocab_size__, \n",
    "                n_embd=bittensor.__network_dim__,\n",
    "                n_layer=config.synapse.n_layer,\n",
    "                n_head=config.synapse.n_head, \n",
    "                n_inner=config.synapse.n_inner, \n",
    "                activation_function=config.synapse.activation_function, \n",
    "                resid_pdrop=config.synapse.resid_pdrop, \n",
    "                embd_pdrop=config.synapse.embd_pdrop, \n",
    "                attn_pdrop=config.synapse.attn_pdrop, \n",
    "                layer_norm_epsilon=config.synapse.layer_norm_epsilon, \n",
    "                initializer_range=config.synapse.initializer_range, \n",
    "                summary_type=config.synapse.summary_type, \n",
    "                summary_use_proj=config.synapse.summary_use_proj, \n",
    "                summary_activation=config.synapse.summary_activation, \n",
    "                summary_proj_to_labels=config.synapse.summary_proj_to_labels, \n",
    "                summary_first_dropout=config.synapse.summary_first_dropout, \n",
    "        )\n",
    "\n",
    "        # encoder_layer: encodes tokenized sequences to network dim.\n",
    "        # [batch_size, sequence_len] -> [batch_size, sequence_len, bittensor.__network_dim__]\n",
    "        self.transformer = GPT2Model(huggingface_config)\n",
    "\n",
    "        # pooler_layer: pools the hidden units for use by the pkm dendrite rpc query.\n",
    "        # [batch_size, bittensor.__network_dim__, sequence_len] -> [batch_size, bittensor.__network_dim__]\n",
    "        self.pooler = GPT2Pooler(huggingface_config)\n",
    "\n",
    "        # router: (PKM layer) queries network using pooled embeddings as context.\n",
    "        # [batch_size, bittensor.__network_dim__] -> topk * [batch_size, bittensor.__network_dim__]\n",
    "        self.router = PKMRouter(config, query_dim = bittensor.__network_dim__)\n",
    "\n",
    "        # hidden_layer: transforms context and encoding to network_dim hidden units.\n",
    "        # [batch_size, sequence_dim, 2 * bittensor.__network_dim__] -> [batch_size, sequence_len, bittensor.__network_dim__]\n",
    "        self.hidden_layer = nn.Linear( bittensor.__network_dim__, bittensor.__network_dim__ )\n",
    "\n",
    "        # target_layer: maps from hidden layer to vocab dimension for each token. Used by MLM loss.\n",
    "        # [batch_size, sequence_len, bittensor.__network_dim__] -> [batch_size, sequence_len, bittensor.__vocab_size__]\n",
    "        self.target_layer = nn.Linear( bittensor.__network_dim__, bittensor.__vocab_size__, bias=False )\n",
    "        \n",
    "        # Loss function: MLM cross-entropy loss.\n",
    "        # predicted: [batch_size, sequence_len, 1], targets: [batch_size, sequence_len, 1] -> [1]\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    @staticmethod   \n",
    "    def default_config() -> Munch:\n",
    "        parser = argparse.ArgumentParser(); \n",
    "        GPT2LMNucleus.add_args(parser) \n",
    "        config = bittensor.config.Config.to_config(parser); \n",
    "        return config\n",
    "\n",
    "    @staticmethod\n",
    "    def add_args(parser: argparse.ArgumentParser):    \n",
    "        r\"\"\" Add custom params to the parser.\n",
    "        \"\"\"\n",
    "        parser.add_argument('--synapse.n_head', default=1, type=int, \n",
    "                            help='Number of attention heads for each attention layer in the Transformer encoder.')\n",
    "        parser.add_argument('--synapse.n_layer', default=2, type=int, \n",
    "                            help='Number of hidden layers in the Transformer encoder.')\n",
    "        parser.add_argument('--synapse.n_inner', default=8, type=int, \n",
    "                            help='The dimensionality of the inner feed-forward layers. :obj:`None` will set it to 4 times n_embd')\n",
    "        parser.add_argument('--synapse.activation_function', default='gelu_new', type=str, \n",
    "                            help='Activation function, to be selected in the list :obj:`[\"relu\", \"silu\", \"gelu\", \"tanh\", \"gelu_new\"]')\n",
    "        parser.add_argument('--synapse.resid_pdrop', default=0.1, type=float, \n",
    "                            help='GPT residual dropout probabilit.')\n",
    "        parser.add_argument('--synapse.embd_pdrop', default=0.1, type=float, \n",
    "                            help='GPT embedding dropout probability.')\n",
    "        parser.add_argument('--synapse.attn_pdrop', default=0.1, type=float, \n",
    "                            help='GPT attention dropout probability.')\n",
    "        parser.add_argument('--synapse.layer_norm_epsilon', default=1e-05, type=float, \n",
    "                            help='GPT the epsilon to use in the layer normalization layers')\n",
    "        parser.add_argument('--synapse.summary_type', default='cls_index', type=str, \n",
    "                            help='Supply a Tensor of classification token position (like GPT/GPT-2).')\n",
    "        parser.add_argument('--synapse.initializer_range', default=0.02, type=float, \n",
    "                            help='The standard deviation of the truncated_normal_initializer for initializing all weight matrices.')\n",
    "        parser.add_argument('--synapse.summary_use_proj', default=True, type=bool, \n",
    "                            help='Whether or not to add a projection after the vector extraction.')\n",
    "        parser.add_argument('--synapse.summary_activation', type=str, \n",
    "                            help='Pass \"tanh\" for a tanh activation to the output, any other value will result in no activation.')\n",
    "        parser.add_argument('--synapse.summary_proj_to_labels', default=True, type=bool, \n",
    "                            help='Whether the projection outputs should have config.num_labels or config.hidden_size classes.')\n",
    "        parser.add_argument('--synapse.summary_first_dropout', default=0.1, type=float, \n",
    "                            help='The dropout ratio to be used after the projection and activation.')\n",
    "        parser.add_argument('--synapse.n_block_filter', default=100, type=int, help='Stale neurons are filtered after this many blocks.')\n",
    "        PKMRouter.add_args(parser)\n",
    "\n",
    "    @staticmethod\n",
    "    def check_config(config: Munch):\n",
    "        pass\n",
    "\n",
    "    def forward_text(self, inputs: torch.LongTensor):\n",
    "        \"\"\" Local forward inputs through the MLM GPT Synapse.\n",
    "\n",
    "            Args:\n",
    "                inputs (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_len)`, `required`): \n",
    "                    Batch_size length list of tokenized sentences.\n",
    "            \n",
    "            Returns:\n",
    "                hidden (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_len, bittensor.__network_dim__)`, `required`): \n",
    "                    Hidden layer representation produced using the local_context.\n",
    "        \"\"\"\n",
    "        hidden = self.local_forward(inputs=inputs.to(self.device), training = False).local_hidden\n",
    "        return hidden\n",
    "\n",
    "    def local_forward(self, inputs: torch.LongTensor, training: bool = True) -> SimpleNamespace:\n",
    "        r\"\"\" Forward pass through GPT synapse.\n",
    "\n",
    "            Args:\n",
    "                inputs (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_len)`, `required`): \n",
    "                    Batch_size length list of text sentences.\n",
    "\n",
    "                training (:obj:`bool')`, `optional`, defaults to True):\n",
    "                    Switch to True if this forward pass computes an MLM loss.\n",
    "\n",
    "            SimpleNamespace {\n",
    "                    local_context (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_len, bittensor.__network_dim__)`, `required`):\n",
    "                        Hidden layer context.\n",
    "\n",
    "                    local_hidden (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_len, bittensor.__network_dim__)`, `required`):\n",
    "                        Hidden layer encoding produced using local_context.\n",
    "\n",
    "                    local_target (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_len, bittensor.__vocab_size__)`, `optional`):\n",
    "                        GPT MLM Target predictions produced using local_context. \n",
    "\n",
    "                    local_target_loss (:obj:`torch.FloatTensor` of shape :obj:`(1)`, `optional`): \n",
    "                        GPT MLM loss using local_context.\n",
    "                }\n",
    "        \"\"\"\n",
    "        inputs = torch.clamp(inputs, 0, bittensor.__vocab_size__) # Filter out of range tokens.\n",
    "\n",
    "        # Return vars to be filled.\n",
    "        output = SimpleNamespace()\n",
    "        \n",
    "        # local_context: distilled version of remote_context.\n",
    "        # local_context.shape = [batch_size, sequence_len, bittensor.__network_dim__]\n",
    "        output.local_context = self.transformer(input_ids=inputs, return_dict=True).last_hidden_state\n",
    "\n",
    "        # local_hidden: hidden layer encoding of sequence with local_context.\n",
    "        # local_hidden.shape = [batch_size, sequence_len, bittensor.__network_dim__]\n",
    "        output.local_hidden = self.hidden_layer(output.local_context)\n",
    "\n",
    "        if training:\n",
    "            # local_target: projection of local_hidden onto target dimension.\n",
    "            # local_target.shape = [batch_size, sequence_len, bittensor.__vocab_size__]\n",
    "            output.local_target = self.target_layer(output.local_hidden)\n",
    "\n",
    "            # local_target_loss: MLM loss between local_target and passed targets.\n",
    "            # local_target_loss.shape = [1]\n",
    "            shift_logits = output.local_target[..., :-1, :].contiguous()\n",
    "            shift_labels = inputs[..., 1:].contiguous()\n",
    "            output.local_target_loss = self.loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                   \n",
    "        return output\n",
    "\n",
    "    def remote_forward(self, neuron: bittensor.neuron.Neuron, inputs: torch.LongTensor, training: bool) -> SimpleNamespace:\n",
    "        \"\"\" Forward pass inputs and labels through the GPT2 module.\n",
    "\n",
    "\n",
    "        Args:\n",
    "            neuron (:obj: `bittensor.neuron.Neuron`, `required`):\n",
    "                    Bittensor neuron, used for making queries to the remote network.\n",
    "\n",
    "            inputs (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_len)`, `required`): \n",
    "                    Batch_size length list of text sentences.\n",
    "\n",
    "            training (:obj:`bool')`, `optional`, defaults to True):\n",
    "                Switch to True if this forward pass computes an MLM loss.\n",
    "\n",
    "        Returns:\n",
    "            self.local_forward() + SimpleNamespace ( \n",
    "\n",
    "                    remote_hidden (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_len, bittensor.__network_dim__)`, `optional`): \n",
    "                        Hidden layer encoding produced using the remote_context.\n",
    "\n",
    "                    remote_target (:obj:`torch.FloatTensor` of shape :obj:`(batch_size,  bittensor.__vocab_size__)`, `optional`):\n",
    "                        GPT MLM Target predictions using the remote_context.\n",
    "\n",
    "                    remote_target_loss (:obj:`torch.FloatTensor` of shape :obj:`(1)`, `optional`):\n",
    "                        GPT MLM loss using the remote_context.\n",
    "\n",
    "                    distillation_loss (:obj:`torch.FloatTensor` of shape :obj:`(1)`, `optional`): \n",
    "                        Distillation loss between local_context and remote_context.\n",
    "\n",
    "                    router (:obj:`SimpleNamespace`, `required`): \n",
    "                        Outputs from the pkm dendrite.\n",
    "            )\n",
    "        \"\"\"\n",
    "        inputs = torch.clamp(inputs, 0, bittensor.__vocab_size__) # Filter out of range tokens.\n",
    "\n",
    "        # Run the local model.\n",
    "        # output = SimpleNamespace\n",
    "        output = self.local_forward(inputs, training)\n",
    "\n",
    "        # pooled: pooled hidden layer from local run, used as our query context.\n",
    "        # pooled.shape = [batch_size, bittensor.__network_dim__]\n",
    "        pooled = self.pooler(output.local_hidden.detach())\n",
    "\n",
    "        # remote_context: joined responses from a dendrite.forward_text call.\n",
    "        # remote_context.shape = [batch_size, sequence_len, bittensor.__network_dim__]\n",
    "        output.router = self.router.forward_text(neuron, inputs.to(self.device), pooled)\n",
    "        remote_context = output.router.response\n",
    "\n",
    "        # distillation_loss: distillation loss between local_context and remote_context\n",
    "        # distillation_loss.shape = [1]\n",
    "        output.distillation_loss = F.mse_loss(output.local_context, remote_context.detach())\n",
    "\n",
    "        # remote_hidden: hidden layer encoding using remote_context.\n",
    "        # remote_hidden.shape = [batch_size, sequence_len, bittensor.__network_dim__]\n",
    "        output.remote_hidden = self.hidden_layer(remote_context)\n",
    "\n",
    "        if training:\n",
    "            # remote_target: projection of remote_hidden onto target dimension.\n",
    "            # remote_target.shape = [batch_size, sequence_len, bittensor.__vocab_size__]\n",
    "            output.remote_target = self.target_layer(output.remote_hidden)\n",
    "\n",
    "            # remote_target_loss: MLM loss between remote_target and passed targets.\n",
    "            # remote_target_loss.shape = [1]\n",
    "            shift_logits = output.remote_target[..., :-1, :].contiguous()\n",
    "            shift_labels = inputs[..., 1:].contiguous()\n",
    "            output.remote_target_loss = self.loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_process( \n",
    "        running_event: mp.Event, \n",
    "        model: 'bittensor.synapse.Synapse',\n",
    "    ):\n",
    "    # Forward process: Answers Forward queries on this endpoint. \n",
    "    while not running_event.is_set():\n",
    "        try:\n",
    "            # Pull next query\n",
    "            print ('got entry')\n",
    "            pong, pubkey, inputs, modality = axon.forward_queue.get(block=True, timeout=5.0)\n",
    "            print ('got request', pubkey, inputs.shape)\n",
    "\n",
    "            # Process request:\n",
    "            try:\n",
    "                outputs = model.forward_text( inputs )\n",
    "                pong.send( outputs )\n",
    "                print ('sent response', pubkey, outputs.shape)\n",
    "            except:\n",
    "                pong.send(None)\n",
    "        except queue.Empty:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAlready subscribed with [ip: 132.191.3.149, port: 8091, modality: 0, coldkey: 0x3c9cd1679888e5660b0c8e4b8a17a1719c0cb7f05b5c624a856b421b52290515]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Subscribing your endpoint.\n",
    "success = subtensor.subscribe(\n",
    "                axon.config.axon.external_ip, \n",
    "                axon.config.axon.external_port,\n",
    "                neuron.config.neuron.modality,\n",
    "                wallet.coldkeypub,\n",
    "                wait_for_finalization = True,\n",
    "                timeout = 4 * bittensor.__blocktime__,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR   |bittensor.axon:_serve:496 - Cannot start already-started server!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GPT2LMSynapse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-44665e1d388a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maxon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMNucleus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-929e72cfee74>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMNucleus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mbittensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_with_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynapse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mGPT2LMSynapse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GPT2LMSynapse' is not defined"
     ]
    }
   ],
   "source": [
    "axon.start()\n",
    "model = GPT2LMNucleus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constumer: Started...\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n",
      "got entry\n"
     ]
    }
   ],
   "source": [
    "print ('Constumer: Started...')\n",
    "event = mp.Event()\n",
    "x = mp.Process( target=forward_process, args=(event, model), daemon=False)\n",
    "x.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "event.set()\n",
    "x.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple training architecture.\n",
    "from typing import Tuple\n",
    "import random\n",
    "\n",
    "def train( model: 'torch.nn.Module' )\n",
    "\n",
    "# Training params.\n",
    "n_steps = 1000\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "momentum = 0.99\n",
    "\n",
    "# Model and optimizer.\n",
    "tokenizer = bittensor.__tokenizer__()\n",
    "model = PoemSentimentClassifier()\n",
    "optimizer = torch.optim.SGD( model.parameters(), lr = learning_rate, momentum = momentum)\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "# Batch iterator: Produces random tokenized batches from the poem dataset.\n",
    "def next_batch(batch_size: int, dataset, tokenizer) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "  inputs = []\n",
    "  targets = []\n",
    "  for i in range(batch_size):\n",
    "    random_index = random.randint(0, len(dataset)-1)\n",
    "    inputs.append( dataset[random_index]['verse_text'] )\n",
    "    targets.append( dataset[random_index]['label'] )\n",
    "  inputs = tokenizer(inputs, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "  targets = torch.tensor( targets, dtype=torch.int64 )\n",
    "  return inputs, targets\n",
    "  \n",
    "# Training loop:\n",
    "for batch_index in range(n_steps):\n",
    "  inputs, targets = next_batch(batch_size, dataset['train'], tokenizer)\n",
    "  logits = model( inputs )\n",
    "  loss = loss_function( logits.view(-1, 4), targets )\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "  print ('step: ', batch_index, ' loss: ', loss.item())\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = bittensor.proto.Neuron(\n",
    "    address = '127.0.0.1',\n",
    "    port = 8091,\n",
    "    public_key = wallet.hotkey.public_key\n",
    ")\n",
    "print ('Endpoint: ', neuron)\n",
    "\n",
    "\n",
    "def producer():\n",
    "    while True:\n",
    "        print ('Procducer: Sending message...')\n",
    "        tensor = torch.tensor([[1,2]], dtype=torch.int64)\n",
    "\n",
    "        responses, codes = dendrite.forward_text(\n",
    "            [neuron],\n",
    "            [tensor]\n",
    "        )\n",
    "        print ('Procducer: Response code:', codes[0])\n",
    "\n",
    "def consumer():\n",
    "    while True:\n",
    "        try:\n",
    "            print ('Consumer: waiting on axon queue')\n",
    "            pong, pubkey, inputs, modality = axon.forward_queue.get(block=True, timeout=120.0)\n",
    "            print ('Consumer: inputs', type(inputs))\n",
    "            print (inputs.shape)\n",
    "            pong.send( torch.zeros([1,2, 512]) )\n",
    "        except queue.Empty:\n",
    "            print ('Consumer: done waiting')\n",
    "\n",
    "print ('Constumer: Started...')\n",
    "x = mp.Process( target=consumer, daemon=False)\n",
    "x.start()\n",
    "\n",
    "print ('Prodcuer: Started...')\n",
    "y = mp.Process( target=producer, daemon=False)\n",
    "y.start()\n",
    "\n",
    "print ('Consumer: Waiting for join.')\n",
    "x.join()\n",
    "print ('Consumer: Joined')\n",
    "\n",
    "print ('Producer: Waiting for join')\n",
    "y.join()\n",
    "print ('Producer: Joined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "class Pooler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pooler, self).__init__()\n",
    "        self.dense = nn.Linear(bittensor.__network_dim__, bittensor.__network_dim__)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor):\n",
    "        # Take last sequence encoding as the sentence's representation.\n",
    "        first_representation = x[:, -1]\n",
    "        pooled_output = self.dense(first_representation)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class PoemSentimentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # For projecting sequences of representations into a single represenation.\n",
    "        self.pooler = Pooler()\n",
    "\n",
    "        # A Feedforward dense layer.\n",
    "        self.hidden = nn.Linear(bittensor.__network_dim__, bittensor.__network_dim__)\n",
    "        \n",
    "        # For projecting our learned feature space onto the target dimension.\n",
    "        self.target = nn.Linear(bittensor.__network_dim__, 4)\n",
    "      \n",
    "        # GPT-distillation model for extracting knowledge from the network.\n",
    "        huggingface_config = GPT2Config(\n",
    "                vocab_size=bittensor.__vocab_size__, \n",
    "                n_embd=bittensor.__network_dim__,\n",
    "        )\n",
    "        self.student = GPT2Model(huggingface_config)\n",
    "        \n",
    "    def forward_text(self, x: torch.LongTensor) -> torch.FloatTensor:\n",
    "        \n",
    "        # Return local hidden output.\n",
    "        return self.local_forward( x ).local_hidden\n",
    "    \n",
    "        \n",
    "    def remote_forward(self, x: torch.LongTensor) -> SimpleNamespace:\n",
    "        output = SimpleNamespace()\n",
    "        \n",
    "        # Remote model.\n",
    "        network_query = [ x for _ in metagraph.neurons]\n",
    "        responses, _ = dendrite.forward_text( metagraph.neurons, network_query )\n",
    "        output.remote_context = torch.mean(torch.stack(responses, dim=2), dim=2) # Average responses.\n",
    " \n",
    "        # Distillation model.\n",
    "        output.local_context = self.student( x )\n",
    "        \n",
    "        # Pooling layer:\n",
    "        # context -> pooled\n",
    "        # [batch_size, sequence_len, network_dim] -> [batch_size, network_dim]\n",
    "        output.local_pooled = self.pooler( local_context )\n",
    "        output.remote_pooled = self.pooler( remote_context )\n",
    "        output.distillation_loss = F.mse_loss(output.local_pooled, output.remote_pooled.detach())\n",
    "        \n",
    "        # Hidden layer\n",
    "        # [batch_size, network_dim] -> [batch_size, network_dim]\n",
    "        output.local_hidden = self.hidden( output.local_pooled )\n",
    "        output.remote_hidden = self.hidden( output.remote_pooled )\n",
    "        \n",
    "        # Target Layer\n",
    "        output.local_logits = self.target( output.local_hidden )\n",
    "        output.remote_logits = self.target( output.remote_hidden )\n",
    "        \n",
    "        # Softmax output\n",
    "        output.local_outputs = F.softmax( output.local_logits, dim=1 )\n",
    "        output.remote_outputs = F.softmax( output.remote_logits, dim=1 )\n",
    "        \n",
    "        return output       \n",
    "    \n",
    "    \n",
    "    def local_forward(self, x: torch.LongTensor) -> SimpleNamespace:\n",
    "\n",
    "        # Distillation model.\n",
    "        output.local_context = self.student( x )\n",
    "        \n",
    "        # Pooling layer.\n",
    "        output.local_pooled = self.pooler( output.local_context )\n",
    "        \n",
    "        # Hidden layer\n",
    "        output.local_hidden = self.hidden( output.local_pooled )\n",
    "        \n",
    "        # Target Layer\n",
    "        output.local_logits = self.target( output.local_hidden )\n",
    "        \n",
    "        # Softmax output\n",
    "        output.local_outputs = F.softmax( output.local_logits, dim=1 )\n",
    "        \n",
    "        return output \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
